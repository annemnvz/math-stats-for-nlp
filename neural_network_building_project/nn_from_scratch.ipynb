{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project on Neural Networks\n",
    "\n",
    "We have a dataset of film reviews classified according to the sentiment they produce, which can be **positive** or **negative**. We want to be able to predict whether a film will make you feel good or not using the vocabulary on the reviews.\n",
    "\n",
    "In order to make your task easier, we have preprocessed the dataset. We want to encode the information of each review as a vector of the most frequent words used in the descriptions of films. As reviews are quite long, we have splitted them into sentences. This decision may distort the feeling of the author of the review, as they can describe both positive and negative aspects of the film. However, being the aim of this project to put into practise the concepts we have learned in this subject, the accuracy of the results play a minor role. \n",
    "\n",
    "The texts are tokenized, and we have filtered out stopwords (except *no* and *not*) and some proper nouns (of the characters). We have also discarded very short sentences (less than 3 words). Then, we have obtained the vocabulary of the most frequent 100 words (this number could be adapted in order to enrich the features used as input).\n",
    "\n",
    "The dataset has been processed in order to be encoded according to the vocabulary. Each column represents a word in the vocabulary. Each row corresponds to a sentence, in which the frequencies of the words in the sentence are given.\n",
    "\n",
    "In the first column you will find the True value (1 = _positive_ or 0 = _negative_) and in the rest of the columns words of the vocabulary (0 = word not appearing; >0 = word frequence in the sentence).\n",
    "\n",
    "The dataset has been splitted into train and test samples. The number of review sentences in the train is **246, out of which 112 are positive and 134 negative**. The number of review sentences in the test is **50, out of which 24 are positive and 26 negative.**\n",
    "\n",
    "Task fullfilled:\n",
    " - Build a neural network which is able to process all the examples on the train and classify them according to the true values.\n",
    " - Consider different values for $\\alpha$ and iterations and choose the best one. \n",
    " - To make a classification of the sentiment of the example, consider that the sentiment is positive when the prediction is bigger than 0.5, and negative otherwise. Considering your last best choices for the weights, what are the predicted sentiments for the test examples? What is the correct classification percentage?\n",
    " - Consider two different structures for the neural network. Repeat the evaluation on the test examples.\n",
    "    \n",
    "(The complete dataset is available at https://github.com/iamtrask/grokking-deep-learning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 246) (246, 1) [1]\n",
      "0    134\n",
      "1    112\n",
      "Name: true_value, dtype: int64\n",
      "Prediction:  [[0.3        0.35714286 0.2        0.35       0.28461538 0.36666667\n",
      "  0.36666667 0.35       0.41666667 0.32       0.3        0.28\n",
      "  0.31111111 0.35       0.3        0.3        0.27857143 0.25\n",
      "  0.25       0.4        0.38       0.375      0.1        0.3\n",
      "  0.23333333 0.275      0.4        0.28       0.36666667 0.3\n",
      "  0.17142857 0.1        0.25555556 0.2        0.44       0.4\n",
      "  0.4        0.3        0.35       0.4        0.3        0.26666667\n",
      "  0.2        0.23333333 0.3        0.175      0.375      0.25714286\n",
      "  0.23333333 0.4        0.28333333 0.25714286 0.22       0.26666667\n",
      "  0.3        0.15       0.32       0.2        0.125      0.36666667\n",
      "  0.36666667 0.27777778 0.275      0.26       0.15       0.22\n",
      "  0.3        0.1        0.23333333 0.25       0.26666667 0.26666667\n",
      "  0.22857143 0.1        0.36666667 0.25       0.25       0.2\n",
      "  0.2        0.35       0.2        0.26666667 0.1        0.35\n",
      "  0.33333333 0.26       0.26666667 0.23333333 0.45       0.2\n",
      "  0.46       0.45       0.35       0.35       0.3        0.35\n",
      "  0.5        0.35       0.2        0.36666667 0.15       0.35\n",
      "  0.1        0.45       0.15       0.3        0.33333333 0.275\n",
      "  0.35       0.425      0.26666667 0.16666667 0.3        0.35\n",
      "  0.23333333 0.23333333 0.25       0.35714286 0.28       0.3\n",
      "  0.2        0.2625     0.33333333 0.25       0.3        0.275\n",
      "  0.3        0.4        0.2        0.225      0.36666667 0.3\n",
      "  0.2        0.33333333 0.26       0.325      0.3        0.25\n",
      "  0.3        0.25       0.3        0.36666667 0.31666667 0.2\n",
      "  0.24       0.2        0.36666667 0.43333333 0.35       0.3\n",
      "  0.27777778 0.41428571 0.4        0.28       0.36666667 0.26\n",
      "  0.23333333 0.175      0.3        0.24       0.32       0.2\n",
      "  0.3        0.25       0.28       0.325      0.125      0.25\n",
      "  0.26666667 0.26666667 0.36666667 0.1        0.25       0.26666667\n",
      "  0.25       0.36666667 0.23333333 0.325      0.275      0.15\n",
      "  0.4        0.1        0.325      0.4        0.35       0.3\n",
      "  0.25       0.3        0.3        0.34285714 0.4        0.3\n",
      "  0.26666667 0.32       0.3        0.35       0.16666667 0.26666667\n",
      "  0.33333333 0.24       0.3        0.25       0.36666667 0.43333333\n",
      "  0.2        0.23333333 0.26666667 0.25       0.25       0.1\n",
      "  0.3        0.13333333 0.33333333 0.2        0.325      0.15\n",
      "  0.45       0.275      0.23333333 0.3        0.26       0.3\n",
      "  0.275      0.3        0.25       0.4        0.35714286 0.325\n",
      "  0.2        0.25       0.35       0.2        0.33333333 0.4\n",
      "  0.275      0.3        0.3        0.4        0.21666667 0.38\n",
      "  0.35       0.35       0.45       0.4        0.31428571 0.2       ]] \n",
      "\n",
      "Gap:\n",
      " [[-0.7        -0.64285714 -0.8        ... -0.6        -0.68571429\n",
      "  -0.8       ]\n",
      " [-0.7        -0.64285714 -0.8        ... -0.6        -0.68571429\n",
      "  -0.8       ]\n",
      " [-0.7        -0.64285714 -0.8        ... -0.6        -0.68571429\n",
      "  -0.8       ]\n",
      " ...\n",
      " [-0.7        -0.64285714 -0.8        ... -0.6        -0.68571429\n",
      "  -0.8       ]\n",
      " [-0.7        -0.64285714 -0.8        ... -0.6        -0.68571429\n",
      "  -0.8       ]\n",
      " [-0.7        -0.64285714 -0.8        ... -0.6        -0.68571429\n",
      "  -0.8       ]] \n",
      "\n",
      "Gap**2:\n",
      "  [[0.49       0.41326531 0.64       ... 0.36       0.47020408 0.64      ]\n",
      " [0.49       0.41326531 0.64       ... 0.36       0.47020408 0.64      ]\n",
      " [0.49       0.41326531 0.64       ... 0.36       0.47020408 0.64      ]\n",
      " ...\n",
      " [0.49       0.41326531 0.64       ... 0.36       0.47020408 0.64      ]\n",
      " [0.49       0.41326531 0.64       ... 0.36       0.47020408 0.64      ]\n",
      " [0.49       0.41326531 0.64       ... 0.36       0.47020408 0.64      ]] \n",
      "\n",
      "Sum Gap**2:  [66.94       63.37755102 77.04       63.735      68.17360947 62.94\n",
      " 62.94       63.735      61.375      65.5104     66.94       68.5664\n",
      " 66.12148148 63.735      66.94       66.94       68.69010204 71.375\n",
      " 71.375      61.76       62.4024     62.59375    92.06       66.94\n",
      " 73.12666667 69.00375    61.76       68.5664     62.94       66.94\n",
      " 80.82938776 92.06       70.82148148 77.04       61.0656     61.76\n",
      " 61.76       66.94       63.735      61.76       66.94       69.76\n",
      " 77.04       73.12666667 66.94       80.33375    62.59375    70.66612245\n",
      " 73.12666667 61.76       68.28166667 70.66612245 74.6264     69.76\n",
      " 66.94       83.935      65.5104     77.04       87.84375    62.94\n",
      " 62.94       68.75925926 69.00375    70.3896     83.935      74.6264\n",
      " 66.94       92.06       73.12666667 71.375      69.76       69.76\n",
      " 73.6522449  92.06       62.94       71.375      71.375      77.04\n",
      " 77.04       63.735      77.04       69.76       92.06       63.735\n",
      " 64.66666667 70.3896     69.76       73.12666667 61.015      77.04\n",
      " 61.0136     61.015      63.735      63.735      66.94       63.735\n",
      " 61.5        63.735      77.04       62.94       83.935      63.735\n",
      " 92.06       61.015      83.935      66.94       64.66666667 69.00375\n",
      " 63.735      61.23375    69.76       81.5        66.94       63.735\n",
      " 73.12666667 73.12666667 71.375      63.37755102 68.5664     66.94\n",
      " 77.04       70.1509375  64.66666667 71.375      66.94       69.00375\n",
      " 66.94       61.76       77.04       74.05375    62.94       66.94\n",
      " 77.04       64.66666667 70.3896     65.18375    66.94       71.375\n",
      " 66.94       71.375      66.94       62.94       65.735      77.04\n",
      " 72.4096     77.04       62.94       61.12666667 63.735      66.94\n",
      " 68.75925926 61.42163265 61.76       68.5664     62.94       70.3896\n",
      " 73.12666667 80.33375    66.94       72.4096     65.5104     77.04\n",
      " 66.94       71.375      68.5664     65.18375    87.84375    71.375\n",
      " 69.76       69.76       62.94       92.06       71.375      69.76\n",
      " 71.375      62.94       73.12666667 65.18375    69.00375    83.935\n",
      " 61.76       92.06       65.18375    61.76       63.735      66.94\n",
      " 71.375      66.94       66.94       64.11755102 61.76       66.94\n",
      " 69.76       65.5104     66.94       63.735      81.5        69.76\n",
      " 64.66666667 72.4096     66.94       71.375      62.94       61.12666667\n",
      " 77.04       73.12666667 69.76       71.375      71.375      92.06\n",
      " 66.94       86.50666667 64.66666667 77.04       65.18375    83.935\n",
      " 61.015      69.00375    73.12666667 66.94       70.3896     66.94\n",
      " 69.00375    66.94       71.375      61.76       63.37755102 65.18375\n",
      " 77.04       71.375      63.735      77.04       64.66666667 61.76\n",
      " 69.00375    66.94       66.94       61.76       75.015      62.4024\n",
      " 63.735      63.735      61.015      61.76       65.89877551 77.04      ] \n",
      "\n",
      "Initial weights = [[0.1 0.2 0.3 0.4 0.5 0.1 0.2 0.3 0.4 0.5 0.1 0.2 0.3 0.4 0.5 0.1 0.2 0.3\n",
      "  0.4 0.5 0.1 0.2 0.3 0.4 0.5 0.1 0.2 0.3 0.4 0.5 0.1 0.2 0.3 0.4 0.5 0.1\n",
      "  0.2 0.3 0.4 0.5 0.1 0.2 0.3 0.4 0.5 0.1 0.2 0.3 0.4 0.5 0.1 0.2 0.3 0.4\n",
      "  0.5 0.1 0.2 0.3 0.4 0.5 0.1 0.2 0.3 0.4 0.5 0.1 0.2 0.3 0.4 0.5 0.1 0.2\n",
      "  0.3 0.4 0.5 0.1 0.2 0.3 0.4 0.5 0.1 0.2 0.3 0.4 0.5 0.1 0.2 0.3 0.4 0.5\n",
      "  0.1 0.2 0.3 0.4 0.5 0.1 0.2 0.3 0.4 0.5]] \n",
      " Initial gap =  [[-0.7        -0.64285714 -0.8        ... -0.6        -0.68571429\n",
      "  -0.8       ]\n",
      " [-0.7        -0.64285714 -0.8        ... -0.6        -0.68571429\n",
      "  -0.8       ]\n",
      " [-0.7        -0.64285714 -0.8        ... -0.6        -0.68571429\n",
      "  -0.8       ]\n",
      " ...\n",
      " [-0.7        -0.64285714 -0.8        ... -0.6        -0.68571429\n",
      "  -0.8       ]\n",
      " [-0.7        -0.64285714 -0.8        ... -0.6        -0.68571429\n",
      "  -0.8       ]\n",
      " [-0.7        -0.64285714 -0.8        ... -0.6        -0.68571429\n",
      "  -0.8       ]] \n",
      " Initial error =  [66.94       63.37755102 77.04       63.735      68.17360947 62.94\n",
      " 62.94       63.735      61.375      65.5104     66.94       68.5664\n",
      " 66.12148148 63.735      66.94       66.94       68.69010204 71.375\n",
      " 71.375      61.76       62.4024     62.59375    92.06       66.94\n",
      " 73.12666667 69.00375    61.76       68.5664     62.94       66.94\n",
      " 80.82938776 92.06       70.82148148 77.04       61.0656     61.76\n",
      " 61.76       66.94       63.735      61.76       66.94       69.76\n",
      " 77.04       73.12666667 66.94       80.33375    62.59375    70.66612245\n",
      " 73.12666667 61.76       68.28166667 70.66612245 74.6264     69.76\n",
      " 66.94       83.935      65.5104     77.04       87.84375    62.94\n",
      " 62.94       68.75925926 69.00375    70.3896     83.935      74.6264\n",
      " 66.94       92.06       73.12666667 71.375      69.76       69.76\n",
      " 73.6522449  92.06       62.94       71.375      71.375      77.04\n",
      " 77.04       63.735      77.04       69.76       92.06       63.735\n",
      " 64.66666667 70.3896     69.76       73.12666667 61.015      77.04\n",
      " 61.0136     61.015      63.735      63.735      66.94       63.735\n",
      " 61.5        63.735      77.04       62.94       83.935      63.735\n",
      " 92.06       61.015      83.935      66.94       64.66666667 69.00375\n",
      " 63.735      61.23375    69.76       81.5        66.94       63.735\n",
      " 73.12666667 73.12666667 71.375      63.37755102 68.5664     66.94\n",
      " 77.04       70.1509375  64.66666667 71.375      66.94       69.00375\n",
      " 66.94       61.76       77.04       74.05375    62.94       66.94\n",
      " 77.04       64.66666667 70.3896     65.18375    66.94       71.375\n",
      " 66.94       71.375      66.94       62.94       65.735      77.04\n",
      " 72.4096     77.04       62.94       61.12666667 63.735      66.94\n",
      " 68.75925926 61.42163265 61.76       68.5664     62.94       70.3896\n",
      " 73.12666667 80.33375    66.94       72.4096     65.5104     77.04\n",
      " 66.94       71.375      68.5664     65.18375    87.84375    71.375\n",
      " 69.76       69.76       62.94       92.06       71.375      69.76\n",
      " 71.375      62.94       73.12666667 65.18375    69.00375    83.935\n",
      " 61.76       92.06       65.18375    61.76       63.735      66.94\n",
      " 71.375      66.94       66.94       64.11755102 61.76       66.94\n",
      " 69.76       65.5104     66.94       63.735      81.5        69.76\n",
      " 64.66666667 72.4096     66.94       71.375      62.94       61.12666667\n",
      " 77.04       73.12666667 69.76       71.375      71.375      92.06\n",
      " 66.94       86.50666667 64.66666667 77.04       65.18375    83.935\n",
      " 61.015      69.00375    73.12666667 66.94       70.3896     66.94\n",
      " 69.00375    66.94       71.375      61.76       63.37755102 65.18375\n",
      " 77.04       71.375      63.735      77.04       64.66666667 61.76\n",
      " 69.00375    66.94       66.94       61.76       75.015      62.4024\n",
      " 63.735      63.735      61.015      61.76       65.89877551 77.04      ] \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Updated weights = [[1.0096486  1.00133312 1.0042309  ... 1.02116326 0.98407005 0.9993358 ]\n",
      " [1.0096486  1.00133312 1.0042309  ... 1.02116326 0.98407005 0.9993358 ]\n",
      " [1.0096486  1.00133312 1.0042309  ... 1.02116326 0.98407005 0.9993358 ]\n",
      " ...\n",
      " [1.0096486  1.00133312 1.0042309  ... 1.02116326 0.98407005 0.9993358 ]\n",
      " [1.0096486  1.00133312 1.0042309  ... 1.02116326 0.98407005 0.9993358 ]\n",
      " [1.0096486  1.00133312 1.0042309  ... 1.02116326 0.98407005 0.9993358 ]] \n",
      "\n",
      "Updated gap= [[ 0.00034159 -0.00214493  0.00541526 ... -0.00203981 -0.00041839\n",
      "   0.00029217]\n",
      " [ 0.00034159 -0.00214493  0.00541526 ... -0.00203981 -0.00041839\n",
      "   0.00029217]\n",
      " [ 0.00034159 -0.00214493  0.00541526 ... -0.00203981 -0.00041839\n",
      "   0.00029217]\n",
      " ...\n",
      " [ 0.00034159 -0.00214493  0.00541526 ... -0.00203981 -0.00041839\n",
      "   0.00029217]\n",
      " [ 0.00034159 -0.00214493  0.00541526 ... -0.00203981 -0.00041839\n",
      "   0.00029217]\n",
      " [ 0.00034159 -0.00214493  0.00541526 ... -0.00203981 -0.00041839\n",
      "   0.00029217]] \n",
      "\n",
      "Updated error =  [1.54321346e-05 5.15702066e-04 3.62034130e-03 1.03594052e-04\n",
      " 5.84740832e-04 1.10670484e-03 3.66879273e-03 5.95689001e-04\n",
      " 8.93575382e-05 5.21578751e-05 7.08229260e-04 3.87035412e-04\n",
      " 6.61685022e-03 1.56500342e-03 4.62940331e-04 1.97716156e-02\n",
      " 5.98973055e-04 3.47323836e-04 1.78363774e-04 2.35759903e-04\n",
      " 4.08277730e-04 1.08516006e-03 8.07788816e-05 7.01285084e-04\n",
      " 5.38988487e-04 4.46492970e-04 4.95414665e-05 5.17579125e-04\n",
      " 5.24184652e-05 2.10907476e-04 3.95265746e-03 1.08110173e-03\n",
      " 2.92332612e-05 3.57355414e-03 4.33804170e-03 2.35771152e-04\n",
      " 1.18114090e-03 2.31304470e-05 9.80398564e-05 6.00975309e-04\n",
      " 1.39702971e-04 9.83358151e-05 3.36669027e-05 2.01453207e-03\n",
      " 1.10701007e-04 9.32422585e-03 2.56720477e-03 4.57981177e-03\n",
      " 7.04608608e-04 4.26746115e-03 2.87104539e-03 1.90466378e-03\n",
      " 1.37841786e-01 2.27015025e-03 1.16975653e-03 7.20410168e-04\n",
      " 3.31235943e-04 1.22263512e-05 2.36791897e-04 3.20463685e-03\n",
      " 4.12494708e-04 9.81309025e-05 8.86313370e-04 1.57220072e-01\n",
      " 2.55436646e-04 5.69670588e-03 1.07072253e-03 2.05314666e-03\n",
      " 4.41117129e-03 1.26517416e-03 1.26045451e-03 1.80741911e-04\n",
      " 1.59630884e-02 3.96196742e-03 6.71786348e-04 3.93510770e-04\n",
      " 6.84489649e-04 1.33496279e-04 2.00129336e-03 9.62501379e-04\n",
      " 2.04452908e-03 3.18786709e-03 3.88541865e-04 2.01601008e-04\n",
      " 8.44268395e-04 1.09110932e-02 3.29606344e-04 5.83350712e-06\n",
      " 1.29773623e-03 6.14057271e-04 8.43126742e-04 2.88191988e-04\n",
      " 1.28810122e-03 1.19679843e-02 2.76358489e-03 3.48064326e-04\n",
      " 5.62652337e-05 8.71965364e-05 1.37127519e-04 5.20277785e-04\n",
      " 3.37066312e-03 2.51524328e-06 2.63516282e-03 1.77614656e-04\n",
      " 4.48991921e-03 3.28852138e-03 1.24946937e-05 2.95969441e-03\n",
      " 1.40876975e-03 5.94367801e-04 1.29028177e-03 1.40613228e-02\n",
      " 1.67365898e-04 1.22749844e-07 3.74821878e-03 1.07997415e-02\n",
      " 3.36422687e-04 1.07627108e-03 3.37100133e-02 8.96153238e-04\n",
      " 3.53734482e-03 3.78099372e-04 4.77939528e-03 8.75823157e-05\n",
      " 9.09410065e-04 4.67640623e-05 8.90158507e-05 2.80678164e-03\n",
      " 9.54466940e-07 1.08756690e-02 3.04389446e-05 1.12325811e-02\n",
      " 2.62046740e-04 1.13140351e-03 5.20528420e-04 4.52281604e-04\n",
      " 1.17480391e-02 8.59965932e-04 3.79231327e-04 5.82892901e-04\n",
      " 1.49949849e-03 4.08045595e-03 2.12534066e-03 1.02947640e-03\n",
      " 4.84346993e-03 3.00683971e-05 1.34129328e-04 6.88761362e-04\n",
      " 1.13860412e-03 2.56446523e-04 7.96734829e-03 2.12931699e-03\n",
      " 1.24060095e-03 1.44622828e-04 2.27973316e-05 3.86325056e-03\n",
      " 1.06965388e-03 1.41601417e-05 3.14428965e-03 5.24580395e-05\n",
      " 7.80027948e-03 3.00683971e-05 1.04444328e-02 2.68130157e-03\n",
      " 4.47538728e-03 1.32551328e-03 3.01050355e-03 4.34304668e-04\n",
      " 7.10428377e-05 6.31286045e-04 1.57854390e-03 3.92973602e-03\n",
      " 2.54481306e-04 1.34265822e-06 5.50014895e-04 1.67397598e-03\n",
      " 6.48018803e-04 6.39057634e-05 1.72754608e-04 5.47822182e-04\n",
      " 6.18073750e-04 9.28718111e-04 6.68141905e-03 7.24501859e-03\n",
      " 3.97416912e-04 1.39145703e-04 9.89080680e-04 1.63732992e-07\n",
      " 2.49349970e-04 1.60128803e-03 6.84098108e-03 1.42015721e-03\n",
      " 2.63755176e-04 1.55333881e-03 6.65327177e-04 2.58851016e-04\n",
      " 7.83448455e-04 1.50574360e-02 2.44273617e-04 8.88480347e-04\n",
      " 1.97307782e-04 2.44557699e-03 5.73647873e-03 3.67066242e-03\n",
      " 1.96944219e-03 1.12046754e-03 3.02730785e-05 2.45574157e-04\n",
      " 1.10133384e-03 9.81511454e-04 3.76090704e-05 2.93545896e-03\n",
      " 4.65244237e-03 8.79426271e-04 1.43492394e-04 1.80910686e-04\n",
      " 3.41276866e-04 3.16583800e-04 1.79675598e-03 4.70813477e-05\n",
      " 2.71945342e-03 1.53363074e-03 3.05555400e-03 4.65072130e-02\n",
      " 1.89061129e-04 1.11479806e-03 5.50137445e-04 6.02653522e-04\n",
      " 2.87493374e-04 2.17069685e-04 3.00746572e-03 1.89141641e-03\n",
      " 3.15816702e-04 3.71889972e-03 2.94493097e-03 1.22008158e-03\n",
      " 3.61261083e-04 7.89102126e-03 4.69295445e-04 3.70495185e-04\n",
      " 4.68904762e-04 4.68904762e-04 2.34831546e-04 3.02744660e-03\n",
      " 5.13506099e-05 2.55648290e-04] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Train\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import random as rd\n",
    "import os\n",
    "import re\n",
    "\n",
    "cwd = os.getcwd() #\n",
    "\n",
    "xy_data = pd.read_csv('./train.csv',sep=\" \") # Examples are defined as TRUE_LABEL REVIEW\n",
    "# TRUE_LABEL is clasified as positive or negative\n",
    "\n",
    "\n",
    "y_val = xy_data.iloc[:,0:1] # only first column\n",
    "x_val = xy_data.iloc[:,1:] # all columns except the first one\n",
    "\n",
    "X,Y = np.array(x_val),np.array(y_val)\n",
    "\n",
    "X = X.T # Each column is a review\n",
    "\n",
    "print(X.shape, Y.shape, Y[0])\n",
    "print(xy_data.iloc[:,0].value_counts())\n",
    "\n",
    "\n",
    "weights = np.array([[0.1], [0.2], [0.3], [0.4], [0.5], [0.1], [0.2], [0.3], [0.4], [0.5],\n",
    "                   [0.1], [0.2], [0.3], [0.4], [0.5], [0.1], [0.2], [0.3], [0.4], [0.5],\n",
    "                   [0.1], [0.2], [0.3], [0.4], [0.5], [0.1], [0.2], [0.3], [0.4], [0.5],\n",
    "                   [0.1], [0.2], [0.3], [0.4], [0.5], [0.1], [0.2], [0.3], [0.4], [0.5],\n",
    "                   [0.1], [0.2], [0.3], [0.4], [0.5], [0.1], [0.2], [0.3], [0.4], [0.5],\n",
    "                   [0.1], [0.2], [0.3], [0.4], [0.5], [0.1], [0.2], [0.3], [0.4], [0.5],\n",
    "                   [0.1], [0.2], [0.3], [0.4], [0.5], [0.1], [0.2], [0.3], [0.4], [0.5],\n",
    "                   [0.1], [0.2], [0.3], [0.4], [0.5], [0.1], [0.2], [0.3], [0.4], [0.5],\n",
    "                   [0.1], [0.2], [0.3], [0.4], [0.5], [0.1], [0.2], [0.3], [0.4], [0.5],\n",
    "                   [0.1], [0.2], [0.3], [0.4], [0.5], [0.1], [0.2], [0.3], [0.4], [0.5]])\n",
    "#ad hoc. I chose to use 100 random weights so each can make a prediciton for the 100 words of the input X)\n",
    "#Then I will just update them to obtaining more accurate real values according to an estimated error of 0.\n",
    "#print (weights.shape)\n",
    "\n",
    "weights = weights.T\n",
    "#print (\"weights\",weights.shape)\n",
    "#I'm converting my weight matrix to vector (1, 100) \n",
    "#so I can multiply it with my input data matrix with dimensions (100, 246)\n",
    "\n",
    "alpha = 0.1 #Random number of alpha\n",
    "Ni =  200  #Random number of iterations\n",
    "input_data = X\n",
    "goal = Y\n",
    "\n",
    "prediction =  np.matmul(weights, input_data)# (1,100)*(100,246)\n",
    "print('Prediction: ', prediction, '\\n')\n",
    "\n",
    "gap = prediction - goal \n",
    "print('Gap:\\n', gap, '\\n')\n",
    "print('Gap**2:\\n ', gap**2, '\\n')\n",
    "print('Sum Gap**2: ', sum(gap**2), '\\n')\n",
    "\n",
    "error = sum(gap**2) \n",
    "print('Initial weights =', weights, '\\n Initial gap = ', gap, '\\n Initial error = ', error, '\\n')\n",
    "\n",
    "Ni = 200\n",
    "# update the weight\n",
    "for iteration in range(Ni):\n",
    "    prediction =  np.matmul(weights, input_data)\n",
    "    gap = prediction - goal\n",
    "    error = sum(gap**2)\n",
    "    weights = weights - alpha*np.matmul(gap, input_data.T)\n",
    "        \n",
    "print('\\n\\n''Updated weights =', weights, '\\n\\n' 'Updated gap=', gap,'\\n\\nUpdated error = ', error, '\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train\n",
    "#I have decided to work with this type of set up where\n",
    "#the error is set to be 0, so I can get the most convenient\n",
    "#value of alpha and number of iterations.\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import random as rd\n",
    "import os\n",
    "import re\n",
    "\n",
    "cwd = os.getcwd() #\n",
    "\n",
    "xy_data = pd.read_csv('./train.csv',sep=\" \") # Examples are defined as TRUE_LABEL REVIEW\n",
    "# TRUE_LABEL is clasified as positive or negative\n",
    "\n",
    "\n",
    "y_val = xy_data.iloc[:,0:1] # only first column\n",
    "x_val = xy_data.iloc[:,1:] # all columns except the first one\n",
    "\n",
    "X,Y = np.array(x_val),np.array(y_val)\n",
    "\n",
    "X = X.T # Each column is a review\n",
    "\n",
    "print(X.shape, Y.shape, Y[0])\n",
    "print(xy_data.iloc[:,0].value_counts())\n",
    "\n",
    "\n",
    "weights = np.array([[0.1], [0.2], [0.3], [0.4], [0.5], [0.1], [0.2], [0.3], [0.4], [0.5],\n",
    "                   [0.1], [0.2], [0.3], [0.4], [0.5], [0.1], [0.2], [0.3], [0.4], [0.5],\n",
    "                   [0.1], [0.2], [0.3], [0.4], [0.5], [0.1], [0.2], [0.3], [0.4], [0.5],\n",
    "                   [0.1], [0.2], [0.3], [0.4], [0.5], [0.1], [0.2], [0.3], [0.4], [0.5],\n",
    "                   [0.1], [0.2], [0.3], [0.4], [0.5], [0.1], [0.2], [0.3], [0.4], [0.5],\n",
    "                   [0.1], [0.2], [0.3], [0.4], [0.5], [0.1], [0.2], [0.3], [0.4], [0.5],\n",
    "                   [0.1], [0.2], [0.3], [0.4], [0.5], [0.1], [0.2], [0.3], [0.4], [0.5],\n",
    "                   [0.1], [0.2], [0.3], [0.4], [0.5], [0.1], [0.2], [0.3], [0.4], [0.5],\n",
    "                   [0.1], [0.2], [0.3], [0.4], [0.5], [0.1], [0.2], [0.3], [0.4], [0.5],\n",
    "                   [0.1], [0.2], [0.3], [0.4], [0.5], [0.1], [0.2], [0.3], [0.4], [0.5]])\n",
    "#ad hoc. I chose to use 100 random weights so each can make a prediciton for the 100 words of the input X)\n",
    "#Then I will just update them to obtaining more accurate real values according to an estimated error of 0.\n",
    "#print (weights.shape)\n",
    "\n",
    "weights = weights.T\n",
    "#print (\"weights\",weights.shape)\n",
    "#I'm converting my weight matrix to vector (1, 100) \n",
    "#so I can multiply it with my input data matrix with dimensions (100, 246)\n",
    "\n",
    "alpha = 0.1 #Random number of alpha\n",
    "Ni =  200  #Random number of iterations\n",
    "for iteration in range(Ni):\n",
    "    error_for_all = 0\n",
    "    for ind in range(246):\n",
    "        input_data = X[:, [ind]] #(100,1[each 1 out of 246]))\n",
    "        #print (\"id\",input_data.shape) \n",
    "        #input_data2= input_data.T\n",
    "        #print (\"id.T\",input_data2.shape) #(1,100)\n",
    "        goal = Y[ind] \n",
    "        #print(\"goal\",goal.shape) (246,1)\n",
    "        prediction = np.matmul(weights, input_data) #multiply each weight with each 1 out of 246 (1,100)*(100,1)\n",
    "        #print(\"pred\", prediction.shape) #(1,1) but there are 246 preds: one x for each weight.\n",
    "        gap = prediction - goal\n",
    "        error = gap**2\n",
    "        weights = weights - alpha*gap*input_data.T\n",
    "        \n",
    "print('\\n\\n''Updated weights =', weights, '\\n\\n' 'Gap=', gap,'\\n\\nError = ', error_for_all, '\\n')\n",
    "print('These are my predictions =\\n', np.matmul(weights, X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 246) (246, 1) [1]\n",
      "0    134\n",
      "1    112\n",
      "Name: true_value, dtype: int64\n",
      "\n",
      "\n",
      "Updated weights= [[ 0.96720321  0.78969208  0.42941048  0.25954953  0.82114487  0.49710724\n",
      "   0.45118091  1.50310527  0.38922573  0.4497097  -0.30628642  0.98444896\n",
      "  -0.07227812 -0.11477675 -0.30294953  0.74904634 -0.33243995  1.30620107\n",
      "   0.2080179   0.81712207 -0.21613236 -0.20382165  0.06351953  1.53253875\n",
      "   0.37124874  0.79029165  0.44814142 -0.52361144  1.33286916  1.35399241\n",
      "   0.34919135  0.94254813 -0.15737753  0.52596146  1.25764323  0.80778765\n",
      "  -0.24680843 -0.58574482  0.5665732   0.70626437 -0.12520672  0.57981076\n",
      "   0.48531307  0.71677958  0.48538636 -0.56033824 -0.45263875 -0.10369534\n",
      "   0.61348077  0.3636277   0.83928301 -0.00809487  0.84299146 -0.37434021\n",
      "   0.50807609 -0.01030918  0.44334335  0.05646798  0.82611093  0.09906427\n",
      "   0.77104309 -0.17655047  0.66111296  1.66417893  0.88765322  0.84350514\n",
      "  -0.16179477  0.37257854 -0.26126199  0.97470676 -0.82700674  0.85414849\n",
      "   0.52441717 -0.44342441 -0.11611649  0.95759595  0.73826641  0.66249857\n",
      "   0.02389421 -0.34837204  0.40752798 -0.30075326 -0.56510436 -0.29644022\n",
      "   1.41867142  0.3615309  -0.43660874  0.3906092   0.05557696 -0.13528799\n",
      "   0.6590943   0.67146876 -0.42635164  0.68993663  1.19537522  0.95564316\n",
      "   0.24621187  0.52942073  0.02562059  1.19487533]] \n",
      "\n",
      "Gap= [[-0.37540794]] \n",
      "\n",
      "Error=  0 \n",
      "\n",
      "These are my predictions =\n",
      " [[ 0.91028575  0.62212388  0.63183362  0.94869646  0.91389776  0.71986561\n",
      "   0.75994872  1.20156019  1.08548954  0.85506907  0.49906255  0.62849893\n",
      "   0.77949858  0.93605109  0.68283345  0.45780251  0.46963742  0.79034199\n",
      "   0.97562334  0.40866462  0.52100472  1.03025322  0.31368639  0.91416964\n",
      "   0.66223515  0.66810462  0.5033683   0.54907866  0.49204951  0.19933175\n",
      "   0.30922337  0.37553543  0.51301028  0.44086794  0.33112409  0.08904176\n",
      "   0.13563119  0.42263452  0.38090214  0.27831434  0.50041029  0.27537112\n",
      "   0.41141907  0.16561002  0.34511852  0.79892279  0.71787007  0.4185675\n",
      "   0.64674138  0.75067213  0.78770828  0.81087047  0.7704151   1.04768794\n",
      "   0.48252487  0.6534716   0.64780776  0.01693925  0.45435288  0.54218309\n",
      "   0.32732896  0.4087619   0.71817223  0.75175916  0.60011362  0.68448741\n",
      "   0.41916774  0.65819728  0.97957838  0.98069904  0.53916477  0.41464062\n",
      "   0.27720915 -0.0389802   0.66382703  0.97714309  0.71140111  1.07274204\n",
      "   0.4454946   0.51476023  0.2309099   0.40600876  0.20343249  0.21285795\n",
      "   0.51195096  0.41333773  0.01587767  0.06053448  0.14584026 -0.13376883\n",
      "   0.42293914  0.24878202  0.32720204  0.18392168  0.7258935   0.55967356\n",
      "   1.19487533  0.50028992  0.51795197  0.90341093  0.70767231  0.7828753\n",
      "   0.85234475  0.50567391  0.87844764  0.42861884  0.41621631  0.5399574\n",
      "   1.07184224  0.47655533  0.85930053  0.44916775  0.63813166  0.50186264\n",
      "   0.46556688  0.64386693  0.68677185  0.42018594  0.83366539  1.10578086\n",
      "   0.36668231  0.57135758  0.35081722  0.58747556  0.31435561  0.64739103\n",
      "   0.46854995  0.43956009  0.10663906 -0.113746    0.65723697  0.18218435\n",
      "   0.36936324  0.324115   -0.02317632  0.15762317  0.26795569  0.11861453\n",
      "   0.4846524   0.04848526  0.14052041  0.02632149  0.32715639  0.11832119\n",
      "   0.11364556  0.06156203 -0.23949318  0.44230851  0.4372694   1.10448154\n",
      "   0.59066033  0.89675832  1.23346662  0.92726185  0.24756816  0.60704597\n",
      "   0.21748272  0.23233012  0.05494039  0.79056874  0.4368651   0.06156203\n",
      "   0.47761254  0.17569077  0.33831125  0.25767699  0.07633896 -0.28431557\n",
      "   0.26666552  0.25666331  0.53764057  0.21825858  0.06717141  0.47933545\n",
      "   0.17421552  0.12057384  0.24976806  0.30644559 -0.13964594  0.18558945\n",
      "   0.16127443  0.54911885  0.25899564  0.44234182 -0.08793932 -0.12414358\n",
      "   0.13420183 -0.27162432  0.49895798  0.2412142   0.13722449 -0.11030768\n",
      "   0.53224234  0.26113607 -0.02496972  0.33697675 -0.25567334 -0.00927176\n",
      "   0.3079344   0.11550346  0.42554336  0.01633716  0.42599403 -0.07346944\n",
      "   0.257054    0.06541969  0.67379071  0.38691667  0.18945139  0.41679004\n",
      "   0.25568595  0.5198926   0.07034155  0.40781341  0.30095462  0.47304769\n",
      "   0.56982316  0.680905    0.51909034  0.57528302  0.60668807  0.47590866\n",
      "  -0.08387131  0.19024965  0.82856992  0.32396228  0.43432945  0.77270988\n",
      "   0.15071687  0.13380786  0.4480841   0.17179753  0.19401084  0.53692031\n",
      "   0.10143633  0.31174483  0.28268821  0.1132356   0.3194051   0.67148313\n",
      "   0.34448001  0.34448001  0.8236279   0.50333632  0.83359565  0.63710565]]\n"
     ]
    }
   ],
   "source": [
    "# Train\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import random as rd\n",
    "import os\n",
    "import re\n",
    "\n",
    "cwd = os.getcwd() \n",
    "\n",
    "xy_data = pd.read_csv('./train.csv',sep=\" \") \n",
    "\n",
    "y_val = xy_data.iloc[:,0:1]\n",
    "x_val = xy_data.iloc[:,1:] \n",
    "\n",
    "X,Y = np.array(x_val),np.array(y_val)\n",
    "\n",
    "X = X.T \n",
    "\n",
    "print(X.shape, Y.shape, Y[0])\n",
    "print(xy_data.iloc[:,0].value_counts())\n",
    "\n",
    "\n",
    "weights = np.array([[0.1], [0.2], [0.3], [0.4], [0.5], [0.1], [0.2], [0.3], [0.4], [0.5],\n",
    "                   [0.1], [0.2], [0.3], [0.4], [0.5], [0.1], [0.2], [0.3], [0.4], [0.5],\n",
    "                   [0.1], [0.2], [0.3], [0.4], [0.5], [0.1], [0.2], [0.3], [0.4], [0.5],\n",
    "                   [0.1], [0.2], [0.3], [0.4], [0.5], [0.1], [0.2], [0.3], [0.4], [0.5],\n",
    "                   [0.1], [0.2], [0.3], [0.4], [0.5], [0.1], [0.2], [0.3], [0.4], [0.5],\n",
    "                   [0.1], [0.2], [0.3], [0.4], [0.5], [0.1], [0.2], [0.3], [0.4], [0.5],\n",
    "                   [0.1], [0.2], [0.3], [0.4], [0.5], [0.1], [0.2], [0.3], [0.4], [0.5],\n",
    "                   [0.1], [0.2], [0.3], [0.4], [0.5], [0.1], [0.2], [0.3], [0.4], [0.5],\n",
    "                   [0.1], [0.2], [0.3], [0.4], [0.5], [0.1], [0.2], [0.3], [0.4], [0.5],\n",
    "                   [0.1], [0.2], [0.3], [0.4], [0.5], [0.1], [0.2], [0.3], [0.4], [0.5]])\n",
    "\n",
    "weights = weights.T\n",
    "\n",
    "#I will now consider different values for ð›¼ and iterations and choose the best one.\n",
    "\n",
    "alpha = 0.1\n",
    "Ni =  37  #Number of iterations\n",
    "for iteration in range(Ni):\n",
    "    error_for_all = 0\n",
    "    for ind in range(246):\n",
    "        input_data = X[:, [ind]]\n",
    "        #print (input_data.shape)\n",
    "        #input_data2= input_data.T\n",
    "        #print (input_data2.shape)\n",
    "        goal = Y[ind]\n",
    "        prediction = np.matmul(weights, input_data)\n",
    "        gap = prediction - goal\n",
    "        error = gap**2\n",
    "        weights = weights - alpha*gap*input_data.T\n",
    "        \n",
    "print('\\n\\n''Updated weights=', weights, '\\n\\n' 'Gap=', gap,'\\n\\nError= ', error_for_all, '\\n')\n",
    "print('These are my predictions =\\n', np.matmul(weights, X))\n",
    "#the same step (0.1) with less iterations shows smaller gap between the prediction and the real output. \n",
    "#Ni = 2000 -> Gap= [[-0.40053022]] \n",
    "#Ni = 200 -> Gap= [[-0.39557737]] \n",
    "#Ni = 20 -> Gap= [[-0.38210846]] \n",
    "#However, if the number of iterations is too small, the gap starts to get bigger again\n",
    "#Ni = 10 -> Gap= [[-0.41339655]] \n",
    "#The best value is found at 38, anything up or down gets us further from the smallest gap at this alpha value.\n",
    "#Ni = 41 -> Gap= [[-0.37539414]] \n",
    "#I will now maintain the number of iterations and vary the alpha value to see which provides better/worse results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 246) (246, 1) [1]\n",
      "0    134\n",
      "1    112\n",
      "Name: true_value, dtype: int64\n",
      "\n",
      "\n",
      "Updated weights = [[ 0.99405306  0.77327022  0.40055255  0.27001113  0.84714474  0.50295956\n",
      "   0.4931697   1.46186834  0.3330859   0.54037277 -0.22912162  0.94219288\n",
      "  -0.08851101 -0.04184842 -0.21641056  0.75360957 -0.24386631  1.27289334\n",
      "   0.2809794   0.67550431 -0.16251248 -0.14534007  0.17534465  1.38677827\n",
      "   0.41899432  0.73613047  0.45802212 -0.2777449   1.23809691  1.27796856\n",
      "   0.36919523  0.85930168 -0.07794687  0.50461242  1.26407484  0.68080584\n",
      "  -0.20407245 -0.44521194  0.50626005  0.68391225 -0.12513564  0.55132426\n",
      "   0.45412857  0.66745158  0.49714838 -0.41665683 -0.35753548  0.04532853\n",
      "   0.55155381  0.43891167  0.78725387  0.00286828  0.75347777 -0.21057813\n",
      "   0.5041742   0.03627095  0.4167948   0.13810232  0.73778035  0.20022163\n",
      "   0.59565478 -0.07811166  0.5575793   1.32542425  0.84744858  0.66005448\n",
      "  -0.09688213  0.38566318 -0.1259109   0.89576743 -0.60809814  0.72911368\n",
      "   0.47731092 -0.30697228  0.00951769  0.71361969  0.64231112  0.61671318\n",
      "   0.09809476 -0.26010105  0.28952698 -0.18569368 -0.38616022 -0.15324072\n",
      "   1.35440064  0.27858139 -0.28250473  0.35945689  0.15517366  0.00825013\n",
      "   0.51414725  0.54321483 -0.22499247  0.63023217  1.00506727  0.79911831\n",
      "   0.23502611  0.43712739  0.15436085  1.15169266]] \n",
      "\n",
      "Gap= [[-0.37026116]] \n",
      "\n",
      "Error =  0 \n",
      "\n",
      "These are my predictions =\n",
      " [[ 8.59782482e-01  6.32280766e-01  6.35250467e-01  8.92237665e-01\n",
      "   8.44906855e-01  7.53397376e-01  7.49596042e-01  1.14829676e+00\n",
      "   1.02342868e+00  8.59766761e-01  5.07237666e-01  5.71038403e-01\n",
      "   7.50380748e-01  8.94820730e-01  6.62030362e-01  4.68053952e-01\n",
      "   4.90179034e-01  7.72803437e-01  9.59945230e-01  4.08910276e-01\n",
      "   5.34069125e-01  1.02095446e+00  2.48771000e-01  8.56695780e-01\n",
      "   5.95720653e-01  6.67873181e-01  5.05902810e-01  5.24542748e-01\n",
      "   5.30508451e-01  2.19520991e-01  3.33343324e-01  4.15770289e-01\n",
      "   4.98890159e-01  4.65201876e-01  3.34814773e-01  1.25026514e-01\n",
      "   1.77976201e-01  4.64221940e-01  3.59374539e-01  3.26612090e-01\n",
      "   4.73917746e-01  2.61876399e-01  4.25770503e-01  2.05936176e-01\n",
      "   3.58123804e-01  7.11349120e-01  7.03092921e-01  4.24263127e-01\n",
      "   6.06581678e-01  7.57321064e-01  7.54822452e-01  7.93116654e-01\n",
      "   7.27179985e-01  9.95209247e-01  4.89591911e-01  6.27289195e-01\n",
      "   5.84315758e-01  4.69786075e-02  4.97061024e-01  5.63936275e-01\n",
      "   3.47054359e-01  4.32837458e-01  7.05731279e-01  7.12785116e-01\n",
      "   6.23389634e-01  5.91671927e-01  4.20525776e-01  6.81624146e-01\n",
      "   9.65743640e-01  9.59771994e-01  5.12623082e-01  4.29758580e-01\n",
      "   3.18038442e-01  7.27557125e-02  6.87828240e-01  9.77519018e-01\n",
      "   6.52401015e-01  1.03007361e+00  4.56287734e-01  5.45848516e-01\n",
      "   2.56094127e-01  4.40017723e-01  2.88698115e-01  2.67457756e-01\n",
      "   5.33827078e-01  4.44943721e-01  4.25131134e-02  7.71120255e-02\n",
      "   1.76011714e-01 -1.19662242e-01  4.23375480e-01  2.73456172e-01\n",
      "   3.90760179e-01  2.40427946e-01  7.40495215e-01  5.15392361e-01\n",
      "   1.15169266e+00  5.11925344e-01  5.04148168e-01  9.18425425e-01\n",
      "   7.26037592e-01  7.71452238e-01  7.76363937e-01  5.00880400e-01\n",
      "   8.83661643e-01  4.14949873e-01  3.65960714e-01  5.24448682e-01\n",
      "   1.02561939e+00  4.71528856e-01  7.92830226e-01  4.63260302e-01\n",
      "   5.82103781e-01  5.20859346e-01  4.63387235e-01  6.42030480e-01\n",
      "   6.58427175e-01  4.22418315e-01  8.13392701e-01  1.11454063e+00\n",
      "   3.19372394e-01  5.44399027e-01  3.48304976e-01  5.78581624e-01\n",
      "   3.59912430e-01  6.35019403e-01  4.20842517e-01  4.70462661e-01\n",
      "   1.19020033e-01 -1.95546472e-02  6.62475275e-01  2.15146998e-01\n",
      "   3.56980802e-01  3.93145860e-01  2.80607324e-02  1.81226238e-01\n",
      "   2.47436653e-01  1.34579452e-01  4.60703614e-01  7.83431229e-02\n",
      "   1.53843753e-01  9.60230606e-02  3.46711222e-01  1.04896468e-01\n",
      "   1.26581244e-01  8.57154670e-02 -1.47984223e-01  4.40470111e-01\n",
      "   3.93607232e-01  1.04526556e+00  5.58334470e-01  8.60443777e-01\n",
      "   1.17552297e+00  9.04652499e-01  2.70207596e-01  6.36421741e-01\n",
      "   2.79386438e-01  2.62208298e-01  1.04339100e-01  7.63090838e-01\n",
      "   4.68398747e-01  8.57154670e-02  4.67101191e-01  1.90337035e-01\n",
      "   3.66595488e-01  2.73707362e-01  1.10143992e-01 -2.16053961e-01\n",
      "   2.98357630e-01  2.93022394e-01  4.86306824e-01  2.15755262e-01\n",
      "   7.24377468e-02  4.74724418e-01  1.84263555e-01  1.54025132e-01\n",
      "   2.60954045e-01  3.30360866e-01 -5.15763323e-02  1.99150555e-01\n",
      "   2.30644177e-01  5.61402399e-01  2.58862221e-01  4.44231735e-01\n",
      "  -2.65371639e-02 -3.97505182e-02  1.41950873e-01 -1.78656588e-01\n",
      "   5.26197564e-01  2.85067246e-01  1.45618738e-01 -3.43598273e-02\n",
      "   5.19307912e-01  3.03365415e-01  3.90253156e-04  3.55036871e-01\n",
      "  -1.94596273e-01  5.49156122e-02  3.49217878e-01  1.56304098e-01\n",
      "   5.01785374e-01  4.82965440e-02  4.35525916e-01 -1.76420203e-02\n",
      "   2.56790937e-01  6.92473769e-02  6.97974671e-01  3.73809764e-01\n",
      "   2.02329343e-01  4.12483572e-01  2.75046559e-01  5.25744827e-01\n",
      "   1.14035558e-01  3.93339432e-01  3.22146703e-01  4.42252005e-01\n",
      "   5.85302471e-01  6.24638152e-01  4.84885898e-01  5.70140541e-01\n",
      "   5.99467129e-01  4.96345677e-01 -1.44189088e-02  2.63375183e-01\n",
      "   8.12142617e-01  3.71313349e-01  4.84400281e-01  7.08568544e-01\n",
      "   1.48173556e-01  1.51835211e-01  4.59982046e-01  1.75547398e-01\n",
      "   2.35925124e-01  4.99873166e-01  1.08579150e-01  3.00231350e-01\n",
      "   2.71682849e-01  1.10358412e-01  2.89329752e-01  6.68027502e-01\n",
      "   3.35281843e-01  3.35281843e-01  7.92462549e-01  4.54590465e-01\n",
      "   8.29789744e-01  6.37514324e-01]]\n"
     ]
    }
   ],
   "source": [
    "# Train\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import random as rd\n",
    "import os\n",
    "import re\n",
    "\n",
    "cwd = os.getcwd() \n",
    "\n",
    "xy_data = pd.read_csv('./train.csv',sep=\" \") \n",
    "\n",
    "y_val = xy_data.iloc[:,0:1]\n",
    "x_val = xy_data.iloc[:,1:] \n",
    "\n",
    "X,Y = np.array(x_val),np.array(y_val)\n",
    "\n",
    "X = X.T \n",
    "\n",
    "print(X.shape, Y.shape, Y[0])\n",
    "print(xy_data.iloc[:,0].value_counts())\n",
    "\n",
    "\n",
    "weights = np.array([[0.1], [0.2], [0.3], [0.4], [0.5], [0.1], [0.2], [0.3], [0.4], [0.5],\n",
    "                   [0.1], [0.2], [0.3], [0.4], [0.5], [0.1], [0.2], [0.3], [0.4], [0.5],\n",
    "                   [0.1], [0.2], [0.3], [0.4], [0.5], [0.1], [0.2], [0.3], [0.4], [0.5],\n",
    "                   [0.1], [0.2], [0.3], [0.4], [0.5], [0.1], [0.2], [0.3], [0.4], [0.5],\n",
    "                   [0.1], [0.2], [0.3], [0.4], [0.5], [0.1], [0.2], [0.3], [0.4], [0.5],\n",
    "                   [0.1], [0.2], [0.3], [0.4], [0.5], [0.1], [0.2], [0.3], [0.4], [0.5],\n",
    "                   [0.1], [0.2], [0.3], [0.4], [0.5], [0.1], [0.2], [0.3], [0.4], [0.5],\n",
    "                   [0.1], [0.2], [0.3], [0.4], [0.5], [0.1], [0.2], [0.3], [0.4], [0.5],\n",
    "                   [0.1], [0.2], [0.3], [0.4], [0.5], [0.1], [0.2], [0.3], [0.4], [0.5],\n",
    "                   [0.1], [0.2], [0.3], [0.4], [0.5], [0.1], [0.2], [0.3], [0.4], [0.5]])\n",
    "\n",
    "weights = weights.T\n",
    "\n",
    "alpha = 0.063 #different tries\n",
    "Ni =  38  #fixed number of iterations\n",
    "for iteration in range(Ni):\n",
    "    error_for_all = 0\n",
    "    for ind in range(246):\n",
    "        input_data = X[:, [ind]]\n",
    "        goal = Y[ind]\n",
    "        prediction = np.matmul(weights, input_data)\n",
    "        gap = prediction - goal\n",
    "        error = gap**2\n",
    "        weights = weights - alpha*gap*input_data.T\n",
    "        \n",
    "print('\\n\\n''Updated weights =', weights, '\\n\\n' 'Gap=', gap,'\\n\\nError = ', error_for_all, '\\n')\n",
    "print('These are my predictions =\\n', np.matmul(weights, X))\n",
    "\n",
    "#different tries for alpha:\n",
    "#alpha = 5 -> Gap= [[-3.63125484e+10]]\n",
    "#alpha = 0.5 -> Gap= [[-0.49070269]]\n",
    "#alpha = 0.05 -> Gap= [[-0.37274581]] \n",
    "#alpha = 0.005 -> Gap= [[-0.60096044]]\n",
    "#the best value is found between 0.05 and 0.005\n",
    "#after different tries, the best alpha value is 0.063. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 246) (246, 1) [1]\n",
      "0    134\n",
      "1    112\n",
      "Name: true_value, dtype: int64\n",
      "\n",
      "\n",
      "Updated weights = [[ 0.99405306  0.77327022  0.40055255  0.27001113  0.84714474  0.50295956\n",
      "   0.4931697   1.46186834  0.3330859   0.54037277 -0.22912162  0.94219288\n",
      "  -0.08851101 -0.04184842 -0.21641056  0.75360957 -0.24386631  1.27289334\n",
      "   0.2809794   0.67550431 -0.16251248 -0.14534007  0.17534465  1.38677827\n",
      "   0.41899432  0.73613047  0.45802212 -0.2777449   1.23809691  1.27796856\n",
      "   0.36919523  0.85930168 -0.07794687  0.50461242  1.26407484  0.68080584\n",
      "  -0.20407245 -0.44521194  0.50626005  0.68391225 -0.12513564  0.55132426\n",
      "   0.45412857  0.66745158  0.49714838 -0.41665683 -0.35753548  0.04532853\n",
      "   0.55155381  0.43891167  0.78725387  0.00286828  0.75347777 -0.21057813\n",
      "   0.5041742   0.03627095  0.4167948   0.13810232  0.73778035  0.20022163\n",
      "   0.59565478 -0.07811166  0.5575793   1.32542425  0.84744858  0.66005448\n",
      "  -0.09688213  0.38566318 -0.1259109   0.89576743 -0.60809814  0.72911368\n",
      "   0.47731092 -0.30697228  0.00951769  0.71361969  0.64231112  0.61671318\n",
      "   0.09809476 -0.26010105  0.28952698 -0.18569368 -0.38616022 -0.15324072\n",
      "   1.35440064  0.27858139 -0.28250473  0.35945689  0.15517366  0.00825013\n",
      "   0.51414725  0.54321483 -0.22499247  0.63023217  1.00506727  0.79911831\n",
      "   0.23502611  0.43712739  0.15436085  1.15169266]] \n",
      "\n",
      "Gap= [[-0.37026116]] \n",
      "\n",
      "Error =  0 \n",
      "\n",
      "These are my predictions =\n",
      " [[ 8.59782482e-01  6.32280766e-01  6.35250467e-01  8.92237665e-01\n",
      "   8.44906855e-01  7.53397376e-01  7.49596042e-01  1.14829676e+00\n",
      "   1.02342868e+00  8.59766761e-01  5.07237666e-01  5.71038403e-01\n",
      "   7.50380748e-01  8.94820730e-01  6.62030362e-01  4.68053952e-01\n",
      "   4.90179034e-01  7.72803437e-01  9.59945230e-01  4.08910276e-01\n",
      "   5.34069125e-01  1.02095446e+00  2.48771000e-01  8.56695780e-01\n",
      "   5.95720653e-01  6.67873181e-01  5.05902810e-01  5.24542748e-01\n",
      "   5.30508451e-01  2.19520991e-01  3.33343324e-01  4.15770289e-01\n",
      "   4.98890159e-01  4.65201876e-01  3.34814773e-01  1.25026514e-01\n",
      "   1.77976201e-01  4.64221940e-01  3.59374539e-01  3.26612090e-01\n",
      "   4.73917746e-01  2.61876399e-01  4.25770503e-01  2.05936176e-01\n",
      "   3.58123804e-01  7.11349120e-01  7.03092921e-01  4.24263127e-01\n",
      "   6.06581678e-01  7.57321064e-01  7.54822452e-01  7.93116654e-01\n",
      "   7.27179985e-01  9.95209247e-01  4.89591911e-01  6.27289195e-01\n",
      "   5.84315758e-01  4.69786075e-02  4.97061024e-01  5.63936275e-01\n",
      "   3.47054359e-01  4.32837458e-01  7.05731279e-01  7.12785116e-01\n",
      "   6.23389634e-01  5.91671927e-01  4.20525776e-01  6.81624146e-01\n",
      "   9.65743640e-01  9.59771994e-01  5.12623082e-01  4.29758580e-01\n",
      "   3.18038442e-01  7.27557125e-02  6.87828240e-01  9.77519018e-01\n",
      "   6.52401015e-01  1.03007361e+00  4.56287734e-01  5.45848516e-01\n",
      "   2.56094127e-01  4.40017723e-01  2.88698115e-01  2.67457756e-01\n",
      "   5.33827078e-01  4.44943721e-01  4.25131134e-02  7.71120255e-02\n",
      "   1.76011714e-01 -1.19662242e-01  4.23375480e-01  2.73456172e-01\n",
      "   3.90760179e-01  2.40427946e-01  7.40495215e-01  5.15392361e-01\n",
      "   1.15169266e+00  5.11925344e-01  5.04148168e-01  9.18425425e-01\n",
      "   7.26037592e-01  7.71452238e-01  7.76363937e-01  5.00880400e-01\n",
      "   8.83661643e-01  4.14949873e-01  3.65960714e-01  5.24448682e-01\n",
      "   1.02561939e+00  4.71528856e-01  7.92830226e-01  4.63260302e-01\n",
      "   5.82103781e-01  5.20859346e-01  4.63387235e-01  6.42030480e-01\n",
      "   6.58427175e-01  4.22418315e-01  8.13392701e-01  1.11454063e+00\n",
      "   3.19372394e-01  5.44399027e-01  3.48304976e-01  5.78581624e-01\n",
      "   3.59912430e-01  6.35019403e-01  4.20842517e-01  4.70462661e-01\n",
      "   1.19020033e-01 -1.95546472e-02  6.62475275e-01  2.15146998e-01\n",
      "   3.56980802e-01  3.93145860e-01  2.80607324e-02  1.81226238e-01\n",
      "   2.47436653e-01  1.34579452e-01  4.60703614e-01  7.83431229e-02\n",
      "   1.53843753e-01  9.60230606e-02  3.46711222e-01  1.04896468e-01\n",
      "   1.26581244e-01  8.57154670e-02 -1.47984223e-01  4.40470111e-01\n",
      "   3.93607232e-01  1.04526556e+00  5.58334470e-01  8.60443777e-01\n",
      "   1.17552297e+00  9.04652499e-01  2.70207596e-01  6.36421741e-01\n",
      "   2.79386438e-01  2.62208298e-01  1.04339100e-01  7.63090838e-01\n",
      "   4.68398747e-01  8.57154670e-02  4.67101191e-01  1.90337035e-01\n",
      "   3.66595488e-01  2.73707362e-01  1.10143992e-01 -2.16053961e-01\n",
      "   2.98357630e-01  2.93022394e-01  4.86306824e-01  2.15755262e-01\n",
      "   7.24377468e-02  4.74724418e-01  1.84263555e-01  1.54025132e-01\n",
      "   2.60954045e-01  3.30360866e-01 -5.15763323e-02  1.99150555e-01\n",
      "   2.30644177e-01  5.61402399e-01  2.58862221e-01  4.44231735e-01\n",
      "  -2.65371639e-02 -3.97505182e-02  1.41950873e-01 -1.78656588e-01\n",
      "   5.26197564e-01  2.85067246e-01  1.45618738e-01 -3.43598273e-02\n",
      "   5.19307912e-01  3.03365415e-01  3.90253156e-04  3.55036871e-01\n",
      "  -1.94596273e-01  5.49156122e-02  3.49217878e-01  1.56304098e-01\n",
      "   5.01785374e-01  4.82965440e-02  4.35525916e-01 -1.76420203e-02\n",
      "   2.56790937e-01  6.92473769e-02  6.97974671e-01  3.73809764e-01\n",
      "   2.02329343e-01  4.12483572e-01  2.75046559e-01  5.25744827e-01\n",
      "   1.14035558e-01  3.93339432e-01  3.22146703e-01  4.42252005e-01\n",
      "   5.85302471e-01  6.24638152e-01  4.84885898e-01  5.70140541e-01\n",
      "   5.99467129e-01  4.96345677e-01 -1.44189088e-02  2.63375183e-01\n",
      "   8.12142617e-01  3.71313349e-01  4.84400281e-01  7.08568544e-01\n",
      "   1.48173556e-01  1.51835211e-01  4.59982046e-01  1.75547398e-01\n",
      "   2.35925124e-01  4.99873166e-01  1.08579150e-01  3.00231350e-01\n",
      "   2.71682849e-01  1.10358412e-01  2.89329752e-01  6.68027502e-01\n",
      "   3.35281843e-01  3.35281843e-01  7.92462549e-01  4.54590465e-01\n",
      "   8.29789744e-01  6.37514324e-01]]\n",
      "New [0.97829312],[0.83158072],[0.46425509],[0.25254652],[0.84236338],[0.49941111],[0.3982277],[1.53827897],[0.48500617],[0.34455053],[-0.38086452],[1.04936081],[0.01939414],[-0.20121832],[-0.34912384],[0.76017423],[-0.37566003],[1.29371956],[0.13744561],[1.09240483],[-0.26255273],[-0.25677293],[-0.12557412],[1.67559589],[0.27950026],[0.82765408],[0.45568791],[-0.92778133],[1.40005674],[1.37127036],[0.33884329],[1.01830059],[-0.20226904],[0.5416187],[1.1846969],[0.97070746],[-0.20186939],[-0.752855],[0.73500703],[0.76656658],[-0.07205643],[0.60872105],[0.49756567],[0.7778359],[0.50010229],[-0.68669364],[-0.5205433],[-0.33079999],[0.69656099],[0.20482022],[0.84356392],[0.0183879],[0.89559422],[-0.59024129],[0.48906595],[-0.01876273],[0.49119977],[-0.09704396],[0.94684656],[-0.10294812],[1.01193132],[-0.37416234],[0.92393849],[2.2195974],[0.94791971],[1.10624846],[-0.17412155],[0.31680436],[-0.46997878],[1.08782986],[-1.06873495],[1.0434044],[0.56842111],[-0.61248971],[-0.2313668],[1.40435686],[0.82124543],[0.68502217],[-0.05442003],[-0.31402426],[0.62669135],[-0.49257749],[-0.76807836],[-0.49902664],[1.42574697],[0.53251532],[-0.61735453],[0.47533006],[-0.05046968],[-0.32304893],[0.92861148],[0.92020826],[-0.81638137],[0.7758058],[1.47809868],[1.14160865],[0.24207072],[0.62760918],[-0.22811186],[1.20714824]\n",
      "Pos [0.85978248]\n",
      "Pos [0.63228077]\n",
      "Pos [0.63525047]\n",
      "Pos [0.89223767]\n",
      "Pos [0.84490685]\n",
      "Pos [0.75339738]\n",
      "Pos [0.74959604]\n",
      "Pos [1.14829676]\n",
      "Pos [1.02342868]\n",
      "Pos [0.85976676]\n",
      "Pos [0.50723767]\n",
      "Pos [0.5710384]\n",
      "Pos [0.75038075]\n",
      "Pos [0.89482073]\n",
      "Pos [0.66203036]\n",
      "Neg [0.46805395]\n",
      "Neg [0.49017903]\n",
      "Pos [0.77280344]\n",
      "Pos [0.95994523]\n",
      "Neg [0.40891028]\n",
      "Pos [0.53406912]\n",
      "Pos [1.02095446]\n",
      "Neg [0.248771]\n",
      "Pos [0.85669578]\n",
      "Pos [0.59572065]\n",
      "Pos [0.66787318]\n",
      "Pos [0.50590281]\n",
      "Pos [0.52454275]\n",
      "Pos [0.53050845]\n",
      "Neg [0.21952099]\n",
      "Neg [0.33334332]\n",
      "Neg [0.41577029]\n",
      "Neg [0.49889016]\n",
      "Neg [0.46520188]\n",
      "Neg [0.33481477]\n",
      "Neg [0.12502651]\n",
      "Neg [0.1779762]\n",
      "Neg [0.46422194]\n",
      "Neg [0.35937454]\n",
      "Neg [0.32661209]\n",
      "Neg [0.47391775]\n",
      "Neg [0.2618764]\n",
      "Neg [0.4257705]\n",
      "Neg [0.20593618]\n",
      "Neg [0.3581238]\n",
      "Pos [0.71134912]\n",
      "Pos [0.70309292]\n",
      "Neg [0.42426313]\n",
      "Pos [0.60658168]\n",
      "Pos [0.75732106]\n",
      "Pos [0.75482245]\n",
      "Pos [0.79311665]\n",
      "Pos [0.72717998]\n",
      "Pos [0.99520925]\n",
      "Neg [0.48959191]\n",
      "Pos [0.6272892]\n",
      "Pos [0.58431576]\n",
      "Neg [0.04697861]\n",
      "Neg [0.49706102]\n",
      "Pos [0.56393628]\n",
      "Neg [0.34705436]\n",
      "Neg [0.43283746]\n",
      "Pos [0.70573128]\n",
      "Pos [0.71278512]\n",
      "Pos [0.62338963]\n",
      "Pos [0.59167193]\n",
      "Neg [0.42052578]\n",
      "Pos [0.68162415]\n",
      "Pos [0.96574364]\n",
      "Pos [0.95977199]\n",
      "Pos [0.51262308]\n",
      "Neg [0.42975858]\n",
      "Neg [0.31803844]\n",
      "Neg [0.07275571]\n",
      "Pos [0.68782824]\n",
      "Pos [0.97751902]\n",
      "Pos [0.65240101]\n",
      "Pos [1.03007361]\n",
      "Neg [0.45628773]\n",
      "Pos [0.54584852]\n",
      "Neg [0.25609413]\n",
      "Neg [0.44001772]\n",
      "Neg [0.28869812]\n",
      "Neg [0.26745776]\n",
      "Pos [0.53382708]\n",
      "Neg [0.44494372]\n",
      "Neg [0.04251311]\n",
      "Neg [0.07711203]\n",
      "Neg [0.17601171]\n",
      "Neg [-0.11966224]\n",
      "Neg [0.42337548]\n",
      "Neg [0.27345617]\n",
      "Neg [0.39076018]\n",
      "Neg [0.24042795]\n",
      "Pos [0.74049521]\n",
      "Pos [0.51539236]\n",
      "Pos [1.15169266]\n",
      "Pos [0.51192534]\n",
      "Pos [0.50414817]\n",
      "Pos [0.91842542]\n",
      "Pos [0.72603759]\n",
      "Pos [0.77145224]\n",
      "Pos [0.77636394]\n",
      "Pos [0.5008804]\n",
      "Pos [0.88366164]\n",
      "Neg [0.41494987]\n",
      "Neg [0.36596071]\n",
      "Pos [0.52444868]\n",
      "Pos [1.02561939]\n",
      "Neg [0.47152886]\n",
      "Pos [0.79283023]\n",
      "Neg [0.4632603]\n",
      "Pos [0.58210378]\n",
      "Pos [0.52085935]\n",
      "Neg [0.46338723]\n",
      "Pos [0.64203048]\n",
      "Pos [0.65842717]\n",
      "Neg [0.42241831]\n",
      "Pos [0.8133927]\n",
      "Pos [1.11454063]\n",
      "Neg [0.31937239]\n",
      "Pos [0.54439903]\n",
      "Neg [0.34830498]\n",
      "Pos [0.57858162]\n",
      "Neg [0.35991243]\n",
      "Pos [0.6350194]\n",
      "Neg [0.42084252]\n",
      "Neg [0.47046266]\n",
      "Neg [0.11902003]\n",
      "Neg [-0.01955465]\n",
      "Pos [0.66247528]\n",
      "Neg [0.215147]\n",
      "Neg [0.3569808]\n",
      "Neg [0.39314586]\n",
      "Neg [0.02806073]\n",
      "Neg [0.18122624]\n",
      "Neg [0.24743665]\n",
      "Neg [0.13457945]\n",
      "Neg [0.46070361]\n",
      "Neg [0.07834312]\n",
      "Neg [0.15384375]\n",
      "Neg [0.09602306]\n",
      "Neg [0.34671122]\n",
      "Neg [0.10489647]\n",
      "Neg [0.12658124]\n",
      "Neg [0.08571547]\n",
      "Neg [-0.14798422]\n",
      "Neg [0.44047011]\n",
      "Neg [0.39360723]\n",
      "Pos [1.04526556]\n",
      "Pos [0.55833447]\n",
      "Pos [0.86044378]\n",
      "Pos [1.17552297]\n",
      "Pos [0.9046525]\n",
      "Neg [0.2702076]\n",
      "Pos [0.63642174]\n",
      "Neg [0.27938644]\n",
      "Neg [0.2622083]\n",
      "Neg [0.1043391]\n",
      "Pos [0.76309084]\n",
      "Neg [0.46839875]\n",
      "Neg [0.08571547]\n",
      "Neg [0.46710119]\n",
      "Neg [0.19033703]\n",
      "Neg [0.36659549]\n",
      "Neg [0.27370736]\n",
      "Neg [0.11014399]\n",
      "Neg [-0.21605396]\n",
      "Neg [0.29835763]\n",
      "Neg [0.29302239]\n",
      "Neg [0.48630682]\n",
      "Neg [0.21575526]\n",
      "Neg [0.07243775]\n",
      "Neg [0.47472442]\n",
      "Neg [0.18426355]\n",
      "Neg [0.15402513]\n",
      "Neg [0.26095405]\n",
      "Neg [0.33036087]\n",
      "Neg [-0.05157633]\n",
      "Neg [0.19915055]\n",
      "Neg [0.23064418]\n",
      "Pos [0.5614024]\n",
      "Neg [0.25886222]\n",
      "Neg [0.44423173]\n",
      "Neg [-0.02653716]\n",
      "Neg [-0.03975052]\n",
      "Neg [0.14195087]\n",
      "Neg [-0.17865659]\n",
      "Pos [0.52619756]\n",
      "Neg [0.28506725]\n",
      "Neg [0.14561874]\n",
      "Neg [-0.03435983]\n",
      "Pos [0.51930791]\n",
      "Neg [0.30336541]\n",
      "Neg [0.00039025]\n",
      "Neg [0.35503687]\n",
      "Neg [-0.19459627]\n",
      "Neg [0.05491561]\n",
      "Neg [0.34921788]\n",
      "Neg [0.1563041]\n",
      "Pos [0.50178537]\n",
      "Neg [0.04829654]\n",
      "Neg [0.43552592]\n",
      "Neg [-0.01764202]\n",
      "Neg [0.25679094]\n",
      "Neg [0.06924738]\n",
      "Pos [0.69797467]\n",
      "Neg [0.37380976]\n",
      "Neg [0.20232934]\n",
      "Neg [0.41248357]\n",
      "Neg [0.27504656]\n",
      "Pos [0.52574483]\n",
      "Neg [0.11403556]\n",
      "Neg [0.39333943]\n",
      "Neg [0.3221467]\n",
      "Neg [0.44225201]\n",
      "Pos [0.58530247]\n",
      "Pos [0.62463815]\n",
      "Neg [0.4848859]\n",
      "Pos [0.57014054]\n",
      "Pos [0.59946713]\n",
      "Neg [0.49634568]\n",
      "Neg [-0.01441891]\n",
      "Neg [0.26337518]\n",
      "Pos [0.81214262]\n",
      "Neg [0.37131335]\n",
      "Neg [0.48440028]\n",
      "Pos [0.70856854]\n",
      "Neg [0.14817356]\n",
      "Neg [0.15183521]\n",
      "Neg [0.45998205]\n",
      "Neg [0.1755474]\n",
      "Neg [0.23592512]\n",
      "Neg [0.49987317]\n",
      "Neg [0.10857915]\n",
      "Neg [0.30023135]\n",
      "Neg [0.27168285]\n",
      "Neg [0.11035841]\n",
      "Neg [0.28932975]\n",
      "Pos [0.6680275]\n",
      "Neg [0.33528184]\n",
      "Neg [0.33528184]\n",
      "Pos [0.79246255]\n",
      "Neg [0.45459047]\n",
      "Pos [0.82978974]\n",
      "Pos [0.63751432]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Negative examples:\n",
      " [[ 4.6805395e-01  4.9017903e-01  4.0891028e-01  2.4877100e-01\n",
      "   2.1952099e-01  3.3334332e-01  4.1577029e-01  4.9889016e-01\n",
      "   4.6520188e-01  3.3481477e-01  1.2502651e-01  1.7797620e-01\n",
      "   4.6422194e-01  3.5937454e-01  3.2661209e-01  4.7391775e-01\n",
      "   2.6187640e-01  4.2577050e-01  2.0593618e-01  3.5812380e-01\n",
      "   4.2426313e-01  4.8959191e-01  4.6978610e-02  4.9706102e-01\n",
      "   3.4705436e-01  4.3283746e-01  4.2052578e-01  4.2975858e-01\n",
      "   3.1803844e-01  7.2755710e-02  4.5628773e-01  2.5609413e-01\n",
      "   4.4001772e-01  2.8869812e-01  2.6745776e-01  4.4494372e-01\n",
      "   4.2513110e-02  7.7112030e-02  1.7601171e-01 -1.1966224e-01\n",
      "   4.2337548e-01  2.7345617e-01  3.9076018e-01  2.4042795e-01\n",
      "   4.1494987e-01  3.6596071e-01  4.7152886e-01  4.6326030e-01\n",
      "   4.6338723e-01  4.2241831e-01  3.1937239e-01  3.4830498e-01\n",
      "   3.5991243e-01  4.2084252e-01  4.7046266e-01  1.1902003e-01\n",
      "  -1.9554650e-02  2.1514700e-01  3.5698080e-01  3.9314586e-01\n",
      "   2.8060730e-02  1.8122624e-01  2.4743665e-01  1.3457945e-01\n",
      "   4.6070361e-01  7.8343120e-02  1.5384375e-01  9.6023060e-02\n",
      "   3.4671122e-01  1.0489647e-01  1.2658124e-01  8.5715470e-02\n",
      "  -1.4798422e-01  4.4047011e-01  3.9360723e-01  2.7020760e-01\n",
      "   2.7938644e-01  2.6220830e-01  1.0433910e-01  4.6839875e-01\n",
      "   8.5715470e-02  4.6710119e-01  1.9033703e-01  3.6659549e-01\n",
      "   2.7370736e-01  1.1014399e-01 -2.1605396e-01  2.9835763e-01\n",
      "   2.9302239e-01  4.8630682e-01  2.1575526e-01  7.2437750e-02\n",
      "   4.7472442e-01  1.8426355e-01  1.5402513e-01  2.6095405e-01\n",
      "   3.3036087e-01 -5.1576330e-02  1.9915055e-01  2.3064418e-01\n",
      "   2.5886222e-01  4.4423173e-01 -2.6537160e-02 -3.9750520e-02\n",
      "   1.4195087e-01 -1.7865659e-01  2.8506725e-01  1.4561874e-01\n",
      "  -3.4359830e-02  3.0336541e-01  3.9025000e-04  3.5503687e-01\n",
      "  -1.9459627e-01  5.4915610e-02  3.4921788e-01  1.5630410e-01\n",
      "   4.8296540e-02  4.3552592e-01 -1.7642020e-02  2.5679094e-01\n",
      "   6.9247380e-02  3.7380976e-01  2.0232934e-01  4.1248357e-01\n",
      "   2.7504656e-01  1.1403556e-01  3.9333943e-01  3.2214670e-01\n",
      "   4.4225201e-01  4.8488590e-01  4.9634568e-01 -1.4418910e-02\n",
      "   2.6337518e-01  3.7131335e-01  4.8440028e-01  1.4817356e-01\n",
      "   1.5183521e-01  4.5998205e-01  1.7554740e-01  2.3592512e-01\n",
      "   4.9987317e-01  1.0857915e-01  3.0023135e-01  2.7168285e-01\n",
      "   1.1035841e-01  2.8932975e-01  3.3528184e-01  3.3528184e-01\n",
      "   4.5459047e-01]]\n",
      "Positive examples:\n",
      " [[0.85978248 0.63228077 0.63525047 0.89223767 0.84490685 0.75339738\n",
      "  0.74959604 1.14829676 1.02342868 0.85976676 0.50723767 0.5710384\n",
      "  0.75038075 0.89482073 0.66203036 0.77280344 0.95994523 0.53406912\n",
      "  1.02095446 0.85669578 0.59572065 0.66787318 0.50590281 0.52454275\n",
      "  0.53050845 0.71134912 0.70309292 0.60658168 0.75732106 0.75482245\n",
      "  0.79311665 0.72717998 0.99520925 0.6272892  0.58431576 0.56393628\n",
      "  0.70573128 0.71278512 0.62338963 0.59167193 0.68162415 0.96574364\n",
      "  0.95977199 0.51262308 0.68782824 0.97751902 0.65240101 1.03007361\n",
      "  0.54584852 0.53382708 0.74049521 0.51539236 1.15169266 0.51192534\n",
      "  0.50414817 0.91842542 0.72603759 0.77145224 0.77636394 0.5008804\n",
      "  0.88366164 0.52444868 1.02561939 0.79283023 0.58210378 0.52085935\n",
      "  0.64203048 0.65842717 0.8133927  1.11454063 0.54439903 0.57858162\n",
      "  0.6350194  0.66247528 1.04526556 0.55833447 0.86044378 1.17552297\n",
      "  0.9046525  0.63642174 0.76309084 0.5614024  0.52619756 0.51930791\n",
      "  0.50178537 0.69797467 0.52574483 0.58530247 0.62463815 0.57014054\n",
      "  0.59946713 0.81214262 0.70856854 0.6680275  0.79246255 0.82978974\n",
      "  0.63751432]]\n"
     ]
    }
   ],
   "source": [
    "#Train\n",
    "#I now perform a sentiment classification of the data according to the predictions obtained in my previous operations.\n",
    "#If the prediction is bigger than 0.5, the example will be classified as positive;\n",
    "#if smaller, the class will be negative.\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import random as rd\n",
    "import os\n",
    "import re\n",
    "\n",
    "cwd = os.getcwd() \n",
    "\n",
    "xy_data = pd.read_csv('./train.csv',sep=\" \") \n",
    "\n",
    "y_val = xy_data.iloc[:,0:1]\n",
    "x_val = xy_data.iloc[:,1:] \n",
    "\n",
    "X,Y = np.array(x_val),np.array(y_val)\n",
    "\n",
    "X = X.T \n",
    "\n",
    "print(X.shape, Y.shape, Y[0])\n",
    "print(xy_data.iloc[:,0].value_counts())\n",
    "\n",
    "\n",
    "weights = np.array([[0.1], [0.2], [0.3], [0.4], [0.5], [0.1], [0.2], [0.3], [0.4], [0.5],\n",
    "                   [0.1], [0.2], [0.3], [0.4], [0.5], [0.1], [0.2], [0.3], [0.4], [0.5],\n",
    "                   [0.1], [0.2], [0.3], [0.4], [0.5], [0.1], [0.2], [0.3], [0.4], [0.5],\n",
    "                   [0.1], [0.2], [0.3], [0.4], [0.5], [0.1], [0.2], [0.3], [0.4], [0.5],\n",
    "                   [0.1], [0.2], [0.3], [0.4], [0.5], [0.1], [0.2], [0.3], [0.4], [0.5],\n",
    "                   [0.1], [0.2], [0.3], [0.4], [0.5], [0.1], [0.2], [0.3], [0.4], [0.5],\n",
    "                   [0.1], [0.2], [0.3], [0.4], [0.5], [0.1], [0.2], [0.3], [0.4], [0.5],\n",
    "                   [0.1], [0.2], [0.3], [0.4], [0.5], [0.1], [0.2], [0.3], [0.4], [0.5],\n",
    "                   [0.1], [0.2], [0.3], [0.4], [0.5], [0.1], [0.2], [0.3], [0.4], [0.5],\n",
    "                   [0.1], [0.2], [0.3], [0.4], [0.5], [0.1], [0.2], [0.3], [0.4], [0.5]])\n",
    "\n",
    "weights = weights.T\n",
    "\n",
    "alpha = 0.063 \n",
    "Ni =  38  \n",
    "for iteration in range(Ni):\n",
    "    error_for_all = 0\n",
    "    for ind in range(246):\n",
    "        input_data = X[:, [ind]]\n",
    "        goal = Y[ind]\n",
    "        prediction = np.matmul(weights, input_data)\n",
    "        gap = prediction - goal\n",
    "        error = gap**2\n",
    "        weights = weights - alpha*gap*input_data.T\n",
    "        \n",
    "print('\\n\\n''Updated weights =', weights, '\\n\\n' 'Gap=', gap,'\\n\\nError = ', error_for_all, '\\n')\n",
    "print('These are my predictions =\\n', np.matmul(weights, X))\n",
    "#print (weights.shape)\n",
    "\n",
    "#weightsnew = weights.T\n",
    "#for i in weightsnew: \n",
    "#    neww = ','.join(str(i) for i in weightsnew)\n",
    "#print(\"New\", neww)   \n",
    "#Here I am just obtaining the values of the new weights so I can use them\n",
    "#later on for the second layer (B) of weights (last question)\n",
    "\n",
    "\n",
    "predictions = np.matmul(weights, X) #these are my latest predictions\n",
    "#print(predictions)\n",
    "#print (predictions.shape)(1, 246)\n",
    "'\\n'\n",
    "predictionsnew = np.array(predictions.T)\n",
    "#print(predictions2)\n",
    "\n",
    "#The predicted sentiments for the test examples, mixed:\n",
    "for i in predictionsnew:\n",
    "    if i <= 0.5:\n",
    "        print (\"Neg\", i)\n",
    "    else:   \n",
    "        print (\"Pos\", i) \n",
    "        \n",
    "#I now separate the negative and the positive sentiment examples\n",
    "\n",
    "#Examples of negative sentiment:\n",
    "for i in predictionsnew: \n",
    "        negs = ','.join(str(i) for i in predictionsnew if i <= 0.5)\n",
    "\n",
    "#print(\"Negatives\", negs) \n",
    "  \n",
    "#I know this is a bit sketchy and I'm sure there is a way to obtain \n",
    "#an array automatically but my python skills are very poor at this moment so\n",
    "#I just printed results above and copied negative values to create a new array with them:\n",
    "negatives = np.array([[0.46805395],[0.49017903],[0.40891028],[0.248771],[0.21952099],[0.33334332],[0.41577029],[0.49889016],[0.46520188],[0.33481477],[0.12502651],[0.1779762],[0.46422194],[0.35937454],[0.32661209],[0.47391775],[0.2618764],[0.4257705],[0.20593618],[0.3581238],[0.42426313],[0.48959191],[0.04697861],[0.49706102],[0.34705436],[0.43283746],[0.42052578],[0.42975858],[0.31803844],[0.07275571],[0.45628773],[0.25609413],[0.44001772],[0.28869812],[0.26745776],[0.44494372],[0.04251311],[0.07711203],[0.17601171],[-0.11966224],[0.42337548],[0.27345617],[0.39076018],[0.24042795],[0.41494987],[0.36596071],[0.47152886],[0.4632603],[0.46338723],[0.42241831],[0.31937239],[0.34830498],[0.35991243],[0.42084252],[0.47046266],[0.11902003],[-0.01955465],[0.215147],[0.3569808],[0.39314586],[0.02806073],[0.18122624],[0.24743665],[0.13457945],[0.46070361],[0.07834312],[0.15384375],[0.09602306],[0.34671122],[0.10489647],[0.12658124],[0.08571547],[-0.14798422],[0.44047011],[0.39360723],[0.2702076],[0.27938644],[0.2622083],[0.1043391],[0.46839875],[0.08571547],[0.46710119],[0.19033703],[0.36659549],[0.27370736],[0.11014399],[-0.21605396],[0.29835763],[0.29302239],[0.48630682],[0.21575526],[0.07243775],[0.47472442],[0.18426355],[0.15402513],[0.26095405],[0.33036087],[-0.05157633],[0.19915055],[0.23064418],[0.25886222],[0.44423173],[-0.02653716],[-0.03975052],[0.14195087],[-0.17865659],[0.28506725],[0.14561874],[-0.03435983],[0.30336541],[0.00039025],[0.35503687],[-0.19459627],[0.05491561],[0.34921788],[0.1563041],[0.04829654],[0.43552592],[-0.01764202],[0.25679094],[0.06924738],[0.37380976],[0.20232934],[0.41248357],[0.27504656],[0.11403556],[0.39333943],[0.3221467],[0.44225201],[0.4848859],[0.49634568],[-0.01441891],[0.26337518],[0.37131335],[0.48440028],[0.14817356],[0.15183521],[0.45998205],[0.1755474],[0.23592512],[0.49987317],[0.10857915],[0.30023135],[0.27168285],[0.11035841],[0.28932975],[0.33528184],[0.33528184],[0.45459047]])\n",
    "negatives = negatives.T #it is now a vector (1, 149)\n",
    "print(\"Negative examples:\"'\\n', negatives)\n",
    "#print (negatives.shape) = (1, 149)\n",
    "\n",
    "\n",
    "#Examples of negative sentiment:\n",
    "for i in predictionsnew: \n",
    "        pos = ','.join(str(i) for i in predictionsnew if i > 0.5)\n",
    "\n",
    "#print(\"Positives\", pos) \n",
    "  \n",
    "#Same here: I printed results above and copied negative values to create a new array with them:\n",
    "positives = np.array([[0.85978248],[0.63228077],[0.63525047],[0.89223767],[0.84490685],[0.75339738],[0.74959604],[1.14829676],[1.02342868],[0.85976676],[0.50723767],[0.5710384],[0.75038075],[0.89482073],[0.66203036],[0.77280344],[0.95994523],[0.53406912],[1.02095446],[0.85669578],[0.59572065],[0.66787318],[0.50590281],[0.52454275],[0.53050845],[0.71134912],[0.70309292],[0.60658168],[0.75732106],[0.75482245],[0.79311665],[0.72717998],[0.99520925],[0.6272892],[0.58431576],[0.56393628],[0.70573128],[0.71278512],[0.62338963],[0.59167193],[0.68162415],[0.96574364],[0.95977199],[0.51262308],[0.68782824],[0.97751902],[0.65240101],[1.03007361],[0.54584852],[0.53382708],[0.74049521],[0.51539236],[1.15169266],[0.51192534],[0.50414817],[0.91842542],[0.72603759],[0.77145224],[0.77636394],[0.5008804],[0.88366164],[0.52444868],[1.02561939],[0.79283023],[0.58210378],[0.52085935],[0.64203048],[0.65842717],[0.8133927],[1.11454063],[0.54439903],[0.57858162],[0.6350194],[0.66247528],[1.04526556],[0.55833447],[0.86044378],[1.17552297],[0.9046525],[0.63642174],[0.76309084],[0.5614024],[0.52619756],[0.51930791],[0.50178537],[0.69797467],[0.52574483],[0.58530247],[0.62463815],[0.57014054],[0.59946713],[0.81214262],[0.70856854],[0.6680275],[0.79246255],[0.82978974],[0.63751432]])\n",
    "positives = positives.T #vector (1, 97)\n",
    "print(\"Positive examples:\"'\\n', positives)\n",
    "#print (positives.shape) = (1, 97)\n",
    "\n",
    "#My new classification percentage is 149 negative examples versus 97 positive examples out of 246,\n",
    "#compared to the initial 134 negative and 112 positive examples.\n",
    "#this means that the relation before was 55% negative and 45% positive, \n",
    "#and after this classification the new percentage is %60.46 negative and %39.43 positive.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 246) (246, 1) [1]\n",
      "0    134\n",
      "1    112\n",
      "Name: true_value, dtype: int64\n",
      "Final weights: [[-0.8344544  -0.7344544  -0.6344544  ... -0.6344544  -0.5344544\n",
      "  -0.4344544 ]\n",
      " [-0.42024932 -0.32024932 -0.22024932 ... -0.22024932 -0.12024932\n",
      "  -0.02024932]\n",
      " [-0.40914053 -0.30914053 -0.20914053 ... -0.20914053 -0.10914053\n",
      "  -0.00914053]\n",
      " ...\n",
      " [-0.4240788  -0.3240788  -0.2240788  ... -0.2240788  -0.1240788\n",
      "  -0.0240788 ]\n",
      " [-0.732438   -0.632438   -0.532438   ... -0.532438   -0.432438\n",
      "  -0.332438  ]\n",
      " [-1.68122441 -1.58122441 -1.48122441 ... -1.48122441 -1.38122441\n",
      "  -1.28122441]] [[ 0.08494386 -0.05230639 -1.21291672 ... -0.87508032 -0.99227694\n",
      "   0.30270963]\n",
      " [ 0.22324179  0.08455697 -0.81386592 ... -0.41799872 -0.87284177\n",
      "   0.44268883]\n",
      " [ 0.56950001  0.42722346  0.23241991 ...  0.96638069 -0.57323699\n",
      "   0.79315646]\n",
      " ...\n",
      " [ 0.41551485  0.27483558 -0.3074792  ...  0.19123093 -0.70651659\n",
      "   0.63729932]\n",
      " [ 1.2221573   1.07311072  2.02049889 ...  2.81496832 -0.01042514\n",
      "   1.45374801]\n",
      " [-0.13078562 -0.2657981  -1.83860628 ... -1.59268436 -1.17859971\n",
      "   0.08435756]]\n",
      "Total error= 11200.0\n",
      "Predictions:\n",
      " [[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# Train\n",
    "#draft.\n",
    "#Here I am just playing with my data to understand what's happening at every step\n",
    "#Results are in the following cell.\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import random as rd\n",
    "import os\n",
    "import re\n",
    "\n",
    "cwd = os.getcwd() \n",
    "\n",
    "xy_data = pd.read_csv('./train.csv',sep=\" \") \n",
    "\n",
    "\n",
    "y_val = xy_data.iloc[:,0:1] \n",
    "x_val = xy_data.iloc[:,1:] \n",
    "\n",
    "X,Y = np.array(x_val),np.array(y_val)\n",
    "\n",
    "X = X.T \n",
    "\n",
    "print(X.shape, Y.shape, Y[0])\n",
    "print(xy_data.iloc[:,0].value_counts())\n",
    "\n",
    "A = np.array([[0.1], [0.2], [0.3], [0.4], [0.5], [0.1], [0.2], [0.3], [0.4], [0.5],\n",
    "              [0.1], [0.2], [0.3], [0.4], [0.5], [0.1], [0.2], [0.3], [0.4], [0.5], \n",
    "              [0.1], [0.2], [0.3], [0.4], [0.5], [0.1], [0.2], [0.3], [0.4], [0.5],\n",
    "              [0.1], [0.2], [0.3], [0.4], [0.5], [0.1], [0.2], [0.3], [0.4], [0.5],\n",
    "              [0.1], [0.2], [0.3], [0.4], [0.5], [0.1], [0.2], [0.3], [0.4], [0.5],\n",
    "              [0.1], [0.2], [0.3], [0.4], [0.5], [0.1], [0.2], [0.3], [0.4], [0.5],\n",
    "              [0.1], [0.2], [0.3], [0.4], [0.5], [0.1], [0.2], [0.3], [0.4], [0.5],\n",
    "              [0.1], [0.2], [0.3], [0.4], [0.5], [0.1], [0.2], [0.3], [0.4], [0.5],\n",
    "              [0.1], [0.2], [0.3], [0.4], [0.5], [0.1], [0.2], [0.3], [0.4], [0.5],\n",
    "              [0.1], [0.2], [0.3], [0.4], [0.5], [0.1], [0.2], [0.3], [0.4], [0.5]])\n",
    "A = A.T\n",
    "#print (A.shape) #(1, 100)\n",
    "#Shape of A = (100, 1) matrix\n",
    "#Shape of A.T = (1, 100) vector\n",
    "\n",
    "#For the second array of weights, I would like to try to use the updated weights\n",
    "#obtained in the previous operations (I figured this is better than using random values)\n",
    "\n",
    "#B weights:\n",
    "B = np.array([[0.97829312],[0.83158072],[0.46425509],[0.25254652],[0.84236338],[0.49941111],[0.3982277],[1.53827897],[0.48500617],[0.34455053],[-0.38086452],[1.04936081],[0.01939414],[-0.20121832],[-0.34912384],[0.76017423],[-0.37566003],[1.29371956],[0.13744561],[1.09240483],[-0.26255273],[-0.25677293],[-0.12557412],[1.67559589],[0.27950026],[0.82765408],[0.45568791],[-0.92778133],[1.40005674],[1.37127036],[0.33884329],[1.01830059],[-0.20226904],[0.5416187],[1.1846969],[0.97070746],[-0.20186939],[-0.752855],[0.73500703],[0.76656658],[-0.07205643],[0.60872105],[0.49756567],[0.7778359],[0.50010229],[-0.68669364],[-0.5205433],[-0.33079999],[0.69656099],[0.20482022],[0.84356392],[0.0183879],[0.89559422],[-0.59024129],[0.48906595],[-0.01876273],[0.49119977],[-0.09704396],[0.94684656],[-0.10294812],[1.01193132],[-0.37416234],[0.92393849],[2.2195974],[0.94791971],[1.10624846],[-0.17412155],[0.31680436],[-0.46997878],[1.08782986],[-1.06873495],[1.0434044],[0.56842111],[-0.61248971],[-0.2313668],[1.40435686],[0.82124543],[0.68502217],[-0.05442003],[-0.31402426],[0.62669135],[-0.49257749],[-0.76807836],[-0.49902664],[1.42574697],[0.53251532],[-0.61735453],[0.47533006],[-0.05046968],[-0.32304893],[0.92861148],[0.92020826],[-0.81638137],[0.7758058],[1.47809868],[1.14160865],[0.24207072],[0.62760918],[-0.22811186],[1.20714824]])\n",
    "#B = B.T\n",
    "#print(B.shape) #(1, 100) with B.T, \n",
    "#Shape of B = (100, 1) matrix\n",
    "#Shape of B.T = (1, 100) vector\n",
    "\n",
    "def ReLU(X): #an activation function defined as the positive part of its argument\n",
    "    return (X >= 0)*X #sets the values that are smaller than 0 to 0\n",
    "def derivReLU(X):\n",
    "    return 1*(X > 0)\n",
    "\n",
    "#print(\"X\", X.shape) \n",
    "#Shape of X = (100, 246) matrix\n",
    "\n",
    "input_data = X\n",
    "layer1 = ReLU(np.matmul(A, input_data)) # Prediction for first layer #A(1,100)*X(100,246)\n",
    "print('First layer: ', layer1 )\n",
    "#print(\"First layer\", layer1.shape) #(1,246). #why no 'for' loop for X here? Wild guess:\n",
    "#because we didn't do the step of taking each X column in a loop, we don't obtain (1,1)preds x 246 times.\n",
    "#we obtain (1,246) because we still have another layer coming up, we need a matrix form to operate with?\n",
    "\n",
    "pred =  np.matmul(B, layer1)   # Prediction for the second layer #B(100,1)*X(1,246)\n",
    "print('Prediction Last layer: ', pred) #my prediction result is a matrix (100,246)\n",
    "#this prediction result is different from the one of only one layer, which was a vector of (1,246) dimensions.\n",
    "\n",
    "goal = Y #Y is (246, 1)\n",
    "gap =  pred.T - goal  #(100,246) - (246,1) cannot do this so I transposed pred: (pred.T) #(246,100)\n",
    "#print(\"gap\", gap.shape) #gap (246, 100)\n",
    "error =  gap**2  \n",
    "print('Gap= ', gap, 'Error = ', error)\n",
    "\n",
    "\n",
    "alpha = 0.063 #I will maintain the original step for now\n",
    "Ni =  38 #maintained for now\n",
    "for iteration in range(Ni):\n",
    "    error_for_all = 0\n",
    "    for ind in range(246):\n",
    "        input_data = X[:, [ind]]\n",
    "        goal = Y[ind]\n",
    "        pre_layer1 =  np.matmul(A, input_data)# <-- Ax product, pre-layer 1\n",
    "        #print(\"prelayer\",pre_layer1.shape) #(1,1)\n",
    "        layer1 =  ReLU(pre_layer1)   # <-- At layer 1\n",
    "        #print(\"layer\",layer1.shape) #dim=(1,1) \n",
    "        pred =  np.matmul(B.T, layer1)    # <-- Prediction at layer 2 #transformed B\n",
    "        #print(\"pred\",pred.shape) #now this is (100,1)\n",
    "        gap = pred - goal\n",
    "        error = gap**2\n",
    "        # Update second layer's weights\n",
    "        B = B - alpha * gap*layer1.T\n",
    "        # Update first layer's weights in two steps\n",
    "        aux = np.multiply(B.T, derivReLU(pre_layer1)) # <-- Multiply arguments element-wise, B and derivReLU evaluated at Ax\n",
    "        #print(\"A\", A.shape)\n",
    "        #A = A.T\n",
    "        #print(\"A.T\", A.shape)\n",
    "        A =  A - alpha * gap* np.matmul(aux, input_data)# <-- update A\n",
    "print('Final weights:', A, B)\n",
    "\n",
    "\n",
    "pred = np.matmul(B, ReLU(np.matmul(A, X)))    # <-- Final predictions\n",
    "gap2 = (pred- Y.T)**2\n",
    "total_error = np.sum(gap2)\n",
    "print('Total error=', total_error)\n",
    "print('Predictions:\\n', pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 246) (246, 1) [1]\n",
      "0    134\n",
      "1    112\n",
      "Name: true_value, dtype: int64\n",
      "Final A weights:\n",
      " [[-0.8344544  -0.7344544  -0.6344544  ... -0.6344544  -0.5344544\n",
      "  -0.4344544 ]\n",
      " [-0.42024932 -0.32024932 -0.22024932 ... -0.22024932 -0.12024932\n",
      "  -0.02024932]\n",
      " [-0.40914053 -0.30914053 -0.20914053 ... -0.20914053 -0.10914053\n",
      "  -0.00914053]\n",
      " ...\n",
      " [-0.4240788  -0.3240788  -0.2240788  ... -0.2240788  -0.1240788\n",
      "  -0.0240788 ]\n",
      " [-0.732438   -0.632438   -0.532438   ... -0.532438   -0.432438\n",
      "  -0.332438  ]\n",
      " [-1.68122441 -1.58122441 -1.48122441 ... -1.48122441 -1.38122441\n",
      "  -1.28122441]] \n",
      " Final B weights:\n",
      " [[ 0.08494386 -0.05230639 -1.21291672 ... -0.87508032 -0.99227694\n",
      "   0.30270963]\n",
      " [ 0.22324179  0.08455697 -0.81386592 ... -0.41799872 -0.87284177\n",
      "   0.44268883]\n",
      " [ 0.56950001  0.42722346  0.23241991 ...  0.96638069 -0.57323699\n",
      "   0.79315646]\n",
      " ...\n",
      " [ 0.41551485  0.27483558 -0.3074792  ...  0.19123093 -0.70651659\n",
      "   0.63729932]\n",
      " [ 1.2221573   1.07311072  2.02049889 ...  2.81496832 -0.01042514\n",
      "   1.45374801]\n",
      " [-0.13078562 -0.2657981  -1.83860628 ... -1.59268436 -1.17859971\n",
      "   0.08435756]]\n",
      "Total error= 11200.0\n",
      "Predictions:\n",
      " [[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "#Train\n",
    "#I will now add a second layer to my initial neural network.\n",
    "#In order to do that, I will add a second array of weights.\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import random as rd\n",
    "import os\n",
    "import re\n",
    "\n",
    "cwd = os.getcwd() \n",
    "\n",
    "xy_data = pd.read_csv('./train.csv',sep=\" \") \n",
    "\n",
    "\n",
    "y_val = xy_data.iloc[:,0:1] \n",
    "x_val = xy_data.iloc[:,1:] \n",
    "\n",
    "X,Y = np.array(x_val),np.array(y_val)\n",
    "\n",
    "X = X.T \n",
    "\n",
    "print(X.shape, Y.shape, Y[0])\n",
    "print(xy_data.iloc[:,0].value_counts())\n",
    "\n",
    "A = np.array([[0.1], [0.2], [0.3], [0.4], [0.5], [0.1], [0.2], [0.3], [0.4], [0.5],\n",
    "              [0.1], [0.2], [0.3], [0.4], [0.5], [0.1], [0.2], [0.3], [0.4], [0.5], \n",
    "              [0.1], [0.2], [0.3], [0.4], [0.5], [0.1], [0.2], [0.3], [0.4], [0.5],\n",
    "              [0.1], [0.2], [0.3], [0.4], [0.5], [0.1], [0.2], [0.3], [0.4], [0.5],\n",
    "              [0.1], [0.2], [0.3], [0.4], [0.5], [0.1], [0.2], [0.3], [0.4], [0.5],\n",
    "              [0.1], [0.2], [0.3], [0.4], [0.5], [0.1], [0.2], [0.3], [0.4], [0.5],\n",
    "              [0.1], [0.2], [0.3], [0.4], [0.5], [0.1], [0.2], [0.3], [0.4], [0.5],\n",
    "              [0.1], [0.2], [0.3], [0.4], [0.5], [0.1], [0.2], [0.3], [0.4], [0.5],\n",
    "              [0.1], [0.2], [0.3], [0.4], [0.5], [0.1], [0.2], [0.3], [0.4], [0.5],\n",
    "              [0.1], [0.2], [0.3], [0.4], [0.5], [0.1], [0.2], [0.3], [0.4], [0.5]])\n",
    "A = A.T\n",
    "#print (A.shape) #(1, 100)\n",
    "#Shape of A = (100, 1) matrix\n",
    "#Shape of A.T = (1, 100) vector\n",
    "\n",
    "#For the second array of weights, I would like to try to use the updated weights\n",
    "#obtained in the previous operations (I figured this is better than using random values)\n",
    "\n",
    "#B weights:\n",
    "B = np.array([[0.97829312],[0.83158072],[0.46425509],[0.25254652],[0.84236338],[0.49941111],[0.3982277],[1.53827897],[0.48500617],[0.34455053],[-0.38086452],[1.04936081],[0.01939414],[-0.20121832],[-0.34912384],[0.76017423],[-0.37566003],[1.29371956],[0.13744561],[1.09240483],[-0.26255273],[-0.25677293],[-0.12557412],[1.67559589],[0.27950026],[0.82765408],[0.45568791],[-0.92778133],[1.40005674],[1.37127036],[0.33884329],[1.01830059],[-0.20226904],[0.5416187],[1.1846969],[0.97070746],[-0.20186939],[-0.752855],[0.73500703],[0.76656658],[-0.07205643],[0.60872105],[0.49756567],[0.7778359],[0.50010229],[-0.68669364],[-0.5205433],[-0.33079999],[0.69656099],[0.20482022],[0.84356392],[0.0183879],[0.89559422],[-0.59024129],[0.48906595],[-0.01876273],[0.49119977],[-0.09704396],[0.94684656],[-0.10294812],[1.01193132],[-0.37416234],[0.92393849],[2.2195974],[0.94791971],[1.10624846],[-0.17412155],[0.31680436],[-0.46997878],[1.08782986],[-1.06873495],[1.0434044],[0.56842111],[-0.61248971],[-0.2313668],[1.40435686],[0.82124543],[0.68502217],[-0.05442003],[-0.31402426],[0.62669135],[-0.49257749],[-0.76807836],[-0.49902664],[1.42574697],[0.53251532],[-0.61735453],[0.47533006],[-0.05046968],[-0.32304893],[0.92861148],[0.92020826],[-0.81638137],[0.7758058],[1.47809868],[1.14160865],[0.24207072],[0.62760918],[-0.22811186],[1.20714824]])\n",
    "B = B.T\n",
    "#print(B.shape) #(1, 100) with B.T, \n",
    "#Shape of B = (1, 100) matrix\n",
    "#Shape of B.T = (100, 1) vector\n",
    "\n",
    "\n",
    "def ReLU(X):\n",
    "    return (X >= 0)*X\n",
    "def derivReLU(X):\n",
    "    return 1*(X > 0)\n",
    "\n",
    "#summary:\n",
    "#print(\"X\", X.shape) #(100,246) #weights should be 246 or 100??\n",
    "#print(\"Y\", Y.shape) #(246,1)\n",
    "#print(\"A\", A.shape) #(1,100)\n",
    "#print(\"B\", B.shape) #(100,1) #Pasar a vector? en GD es 1,4.\n",
    "#la Ãºnica diferencia en la rel dimensional es Y y B. Si se transposa B\n",
    "#la Ãºnica diferencia es 246 y 100. Y= (246,1) y B =(1, 100).\n",
    "\n",
    "alpha = 0.063 #I will maintain the original step for now\n",
    "Ni =  38 #maintained for now\n",
    "\n",
    "for iteration in range(Ni):\n",
    "    for ind in range(246):\n",
    "        input_data = X[:, [ind]] #(100,1[each 1 out of 246]))\n",
    "        goal = Y[ind] #(246,1)\n",
    "        pre_layer1 =  np.matmul(A, input_data)#(1,100)*(100,1)[*246 times]\n",
    "        #print(\"pre_layer\",pre_layer1) [[0.3]]\n",
    "        #print(\"pred1\", pre_layer1.shape) #Pre_layer1=(1,1) but there are 246 preds: one x for each weight.\n",
    "        layer1 =  ReLU(pre_layer1)   #(1,1)*[246 times]\n",
    "        #print(\"layer1\",layer1) #layer1 [[0.3]]\n",
    "        pred =  np.matmul(B.T, layer1)    #(100,1)*(1,1) = (100,1)\n",
    "        #print(\"pred\",pred.shape) #Pred=(100,1)*[246 times]\n",
    "        gap = pred - goal #(100,1)-(246,1)\n",
    "        error = gap**2\n",
    "        # Updating second layer's weights\n",
    "        B = B - alpha * gap*layer1.T\n",
    "        # Updating first layer's weights in two steps\n",
    "        aux = np.multiply(B.T, derivReLU(pre_layer1)) #Multiply arguments element-wise, B and derivReLU evaluated at Ax\n",
    "        A =  A - alpha * gap* np.matmul(aux, input_data)#update A\n",
    "print('Final A weights:\\n', A,'\\n',\n",
    "      'Final B weights:\\n', B)\n",
    "\n",
    "pred = np.matmul(B, ReLU(np.matmul(A, X)))#final predictions\n",
    "gap2 = (pred- Y.T)**2\n",
    "total_error = np.sum(gap2)\n",
    "print('Total error=', total_error)\n",
    "print('Predictions for train set:\\n', pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 50) (50, 1) [1]\n",
      "0    26\n",
      "1    24\n",
      "Name: true_value, dtype: int64\n",
      "Prediction:  [[0.34       0.46666667 0.2        0.35       0.3        0.3\n",
      "  0.43333333 0.3        0.15       0.3        0.31666667 0.23333333\n",
      "  0.38571429 0.175      0.4        0.3        0.18       0.22857143\n",
      "  0.28333333 0.26666667 0.32       0.25       0.25       0.225\n",
      "  0.25       0.4        0.23333333 0.24285714 0.35       0.33333333\n",
      "  0.225      0.3        0.24285714 0.2875     0.2        0.35\n",
      "  0.4        0.25       0.3        0.3        0.4        0.26666667\n",
      "  0.35       0.35       0.26666667 0.25       0.22       0.23333333\n",
      "  0.3        0.225     ]] \n",
      "\n",
      "Gap:\n",
      " [[-0.66       -0.53333333 -0.8        ... -0.76666667 -0.7\n",
      "  -0.775     ]\n",
      " [ 0.34        0.46666667  0.2        ...  0.23333333  0.3\n",
      "   0.225     ]\n",
      " [ 0.34        0.46666667  0.2        ...  0.23333333  0.3\n",
      "   0.225     ]\n",
      " ...\n",
      " [ 0.34        0.46666667  0.2        ...  0.23333333  0.3\n",
      "   0.225     ]\n",
      " [-0.66       -0.53333333 -0.8        ... -0.76666667 -0.7\n",
      "  -0.775     ]\n",
      " [-0.66       -0.53333333 -0.8        ... -0.76666667 -0.7\n",
      "  -0.775     ]] \n",
      "\n",
      "Gap**2:\n",
      "  [[0.4356     0.28444444 0.64       ... 0.58777778 0.49       0.600625  ]\n",
      " [0.1156     0.21777778 0.04       ... 0.05444444 0.09       0.050625  ]\n",
      " [0.1156     0.21777778 0.04       ... 0.05444444 0.09       0.050625  ]\n",
      " ...\n",
      " [0.1156     0.21777778 0.04       ... 0.05444444 0.09       0.050625  ]\n",
      " [0.4356     0.28444444 0.64       ... 0.58777778 0.49       0.600625  ]\n",
      " [0.4356     0.28444444 0.64       ... 0.58777778 0.49       0.600625  ]] \n",
      "\n",
      "Sum Gap**2:  [13.46       12.48888889 16.4        13.325      14.1        14.1\n",
      " 12.58888889 14.1        17.925      14.1        13.81388889 15.52222222\n",
      " 12.9244898  17.13125    12.8        14.1        16.98       15.64081633\n",
      " 14.41388889 14.75555556 13.76       15.125      15.125      15.73125\n",
      " 15.125      12.8        15.52222222 15.29183673 13.325      13.55555556\n",
      " 15.73125    14.1        15.29183673 14.3328125  16.4        13.325\n",
      " 12.8        15.125      14.1        14.1        12.8        14.75555556\n",
      " 13.325      13.325      14.75555556 15.125      15.86       15.52222222\n",
      " 14.1        15.73125   ] \n",
      "\n",
      "\n",
      "\n",
      "Updated weights = [[ 2.29007075  1.29575571  0.26587925 ...  0.3        -0.25572847\n",
      "   1.15889039]\n",
      " [ 2.40664869  1.29199181  0.26089705 ...  0.3        -0.22917167\n",
      "   1.09029822]\n",
      " [ 2.40664869  1.29199181  0.26089705 ...  0.3        -0.22917167\n",
      "   1.09029822]\n",
      " ...\n",
      " [ 2.40664869  1.29199181  0.26089705 ...  0.3        -0.22917167\n",
      "   1.09029822]\n",
      " [ 2.29007075  1.29575571  0.26587925 ...  0.3        -0.25572847\n",
      "   1.15889039]\n",
      " [ 2.29007075  1.29575571  0.26587925 ...  0.3        -0.25572847\n",
      "   1.15889039]] \n",
      "\n",
      "Gap= [[-0.03557972]\n",
      " [-0.04904425]\n",
      " [-0.04904425]\n",
      " [-0.04904425]\n",
      " [-0.04904425]\n",
      " [-0.04904425]\n",
      " [-0.04904425]\n",
      " [-0.03557972]\n",
      " [-0.04904425]\n",
      " [-0.03557972]\n",
      " [-0.04904425]\n",
      " [-0.03557972]\n",
      " [-0.04904425]\n",
      " [-0.04904425]\n",
      " [-0.04904425]\n",
      " [-0.04904425]\n",
      " [-0.04904425]\n",
      " [-0.04904425]\n",
      " [-0.03557972]\n",
      " [-0.03557972]\n",
      " [-0.03557972]\n",
      " [-0.04904425]\n",
      " [-0.04904425]\n",
      " [-0.04904425]\n",
      " [-0.04904425]\n",
      " [-0.04904425]\n",
      " [-0.03557972]\n",
      " [-0.03557972]\n",
      " [-0.04904425]\n",
      " [-0.03557972]\n",
      " [-0.03557972]\n",
      " [-0.04904425]\n",
      " [-0.03557972]\n",
      " [-0.03557972]\n",
      " [-0.03557972]\n",
      " [-0.04904425]\n",
      " [-0.04904425]\n",
      " [-0.03557972]\n",
      " [-0.04904425]\n",
      " [-0.03557972]\n",
      " [-0.03557972]\n",
      " [-0.04904425]\n",
      " [-0.03557972]\n",
      " [-0.03557972]\n",
      " [-0.03557972]\n",
      " [-0.03557972]\n",
      " [-0.03557972]\n",
      " [-0.04904425]\n",
      " [-0.03557972]\n",
      " [-0.03557972]] \n",
      "\n",
      "Error =  0 \n",
      "\n",
      "These are my predictions =\n",
      " [[0.92962107 0.07870142 0.01939141 ... 0.03268933 0.96935195 0.96575452]\n",
      " [0.93927781 0.0649233  0.01736696 ... 0.02769804 0.97670885 0.95279491]\n",
      " [0.93927781 0.0649233  0.01736696 ... 0.02769804 0.97670885 0.95279491]\n",
      " ...\n",
      " [0.93927781 0.0649233  0.01736696 ... 0.02769804 0.97670885 0.95279491]\n",
      " [0.92962107 0.07870142 0.01939141 ... 0.03268933 0.96935195 0.96575452]\n",
      " [0.92962107 0.07870142 0.01939141 ... 0.03268933 0.96935195 0.96575452]]\n"
     ]
    }
   ],
   "source": [
    "# Test\n",
    "#I now apply the previous procedure to our examples of the test set.\n",
    "#In this section, I focus on only one layer of NN and \n",
    "#I obtain best alpha values and number of iterations\n",
    "#A second NN structure is added in the following cell.\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import random as rd\n",
    "import os\n",
    "import re\n",
    "\n",
    "cwd = os.getcwd() \n",
    "\n",
    "xy_data = pd.read_csv('./test.csv',sep=\" \") \n",
    "\n",
    "y_val = xy_data.iloc[:,0:1]\n",
    "x_val = xy_data.iloc[:,1:] \n",
    "\n",
    "X,Y = np.array(x_val),np.array(y_val)\n",
    "\n",
    "X = X.T \n",
    "\n",
    "print(X.shape, Y.shape, Y[0])\n",
    "print(xy_data.iloc[:,0].value_counts())\n",
    "\n",
    "\n",
    "weights = np.array([[0.1], [0.2], [0.3], [0.4], [0.5], [0.1], [0.2], [0.3], [0.4], [0.5],\n",
    "                   [0.1], [0.2], [0.3], [0.4], [0.5], [0.1], [0.2], [0.3], [0.4], [0.5],\n",
    "                   [0.1], [0.2], [0.3], [0.4], [0.5], [0.1], [0.2], [0.3], [0.4], [0.5],\n",
    "                   [0.1], [0.2], [0.3], [0.4], [0.5], [0.1], [0.2], [0.3], [0.4], [0.5],\n",
    "                   [0.1], [0.2], [0.3], [0.4], [0.5], [0.1], [0.2], [0.3], [0.4], [0.5],\n",
    "                   [0.1], [0.2], [0.3], [0.4], [0.5], [0.1], [0.2], [0.3], [0.4], [0.5],\n",
    "                   [0.1], [0.2], [0.3], [0.4], [0.5], [0.1], [0.2], [0.3], [0.4], [0.5],\n",
    "                   [0.1], [0.2], [0.3], [0.4], [0.5], [0.1], [0.2], [0.3], [0.4], [0.5],\n",
    "                   [0.1], [0.2], [0.3], [0.4], [0.5], [0.1], [0.2], [0.3], [0.4], [0.5],\n",
    "                   [0.1], [0.2], [0.3], [0.4], [0.5], [0.1], [0.2], [0.3], [0.4], [0.5]])\n",
    "#Same first 100 random weights to make a prediciton.\n",
    "\n",
    "weights = weights.T\n",
    "alpha = 0.1 #Random number of alpha\n",
    "Ni =  200  #Random number of iterations\n",
    "input_data = X\n",
    "goal = Y\n",
    "\n",
    "prediction =  np.matmul(weights, input_data)# (1,100)*(100,246)\n",
    "print('Prediction: ', prediction, '\\n')\n",
    "\n",
    "gap = prediction - goal \n",
    "print('Gap:\\n', gap, '\\n')\n",
    "print('Gap**2:\\n ', gap**2, '\\n')\n",
    "print('Sum Gap**2: ', sum(gap**2), '\\n')\n",
    "\n",
    "error = sum(gap**2) \n",
    "#print('Initial weights =', weights, '\\n Initial gap = ', gap, '\\n Initial error = ', error, '\\n')\n",
    "\n",
    "Ni = 200\n",
    "# update the weight\n",
    "for iteration in range(Ni):\n",
    "    prediction =  np.matmul(weights, input_data)\n",
    "    gap = prediction - goal\n",
    "    error = sum(gap**2)\n",
    "    weights = weights - alpha*np.matmul(gap, input_data.T)\n",
    "        \n",
    "#print('\\n\\n''Updated weights =', weights, '\\n\\n' 'Updated gap=', gap,'\\n\\nUpdated error = ', error, '\\n')\n",
    "\n",
    "#Trying different alpha and Ni values with total error of 0:\n",
    "alpha = 4 #After several tries (see below), I found this to be the best alpha value\n",
    "Ni =  300  #Best result for number of iterations\n",
    "for iteration in range(Ni):\n",
    "    error_for_all = 0\n",
    "    for ind in range(50):\n",
    "        input_data = X[:, [ind]]\n",
    "        goal = Y[ind] \n",
    "        prediction = np.matmul(weights, input_data)\n",
    "        gap = prediction - goal\n",
    "        error = gap**2\n",
    "        weights = weights - alpha*gap*input_data.T\n",
    "        \n",
    "#different tries for alpha:\n",
    "#alpha = 5 -> Gap= [[1.45710669e+27]]\n",
    "#alpha = 0.5 -> Gap= [[-0.00574624]]\n",
    "#alpha = 0.05 -> Gap= [[-0.05765209]]\n",
    "#alpha = 0.005 -> Gap= [[-0.60096044]\n",
    "#alpha = 0.6 -> Gap= [[-0.00392432]]\n",
    "#alpha = 0.8 -> Gap= [[-0.00212213]]\n",
    "#alpha = 4 -> Gap= [[-1.65472469e-09]]\n",
    "\n",
    "#different tries for Ni:\n",
    "#Ni = 200 -> Gap= [[-1.65472469e-09]]\n",
    "#Ni = 20 -> Gap= [[-0.0022714]] \n",
    "#Ni = 335 -> Gap= [[-5.7620575e-14]] #Increasing more gives RuntimeWarning: overflow error\n",
    "\n",
    "\n",
    "print('\\n\\n''Updated weights =', weights, '\\n\\n' 'Gap=', gap,'\\n\\nError = ', error_for_all, '\\n')\n",
    "print('These are my predictions =\\n', np.matmul(weights, X))\n",
    "\n",
    "#obtaining updated weights value for my second NN layer:\n",
    "#weightsnew = weights.T\n",
    "#for i in weightsnew: \n",
    "#    neww = ','.join(str(i) for i in weightsnew)\n",
    "#print(\"New\", neww)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 50) (50, 1) [1]\n",
      "0    26\n",
      "1    24\n",
      "Name: true_value, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-19-55420ea780a1>:72: RuntimeWarning: overflow encountered in square\n",
      "  error = gap**2\n",
      "<ipython-input-19-55420ea780a1>:74: RuntimeWarning: overflow encountered in multiply\n",
      "  B = B - alpha * gap*layer1.T\n",
      "<ipython-input-19-55420ea780a1>:77: RuntimeWarning: invalid value encountered in matmul\n",
      "  A =  A - alpha * gap* np.matmul(aux, input_data)#update A\n",
      "<ipython-input-19-55420ea780a1>:70: RuntimeWarning: invalid value encountered in matmul\n",
      "  pred =  np.matmul(B.T, layer1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final A weights:\n",
      " [[nan nan nan ... nan nan nan]\n",
      " [nan nan nan ... nan nan nan]\n",
      " [nan nan nan ... nan nan nan]\n",
      " ...\n",
      " [nan nan nan ... nan nan nan]\n",
      " [nan nan nan ... nan nan nan]\n",
      " [nan nan nan ... nan nan nan]] \n",
      " Final B weights:\n",
      " [[nan nan nan ... nan nan nan]\n",
      " [nan nan nan ... nan nan nan]\n",
      " [nan nan nan ... nan nan nan]\n",
      " ...\n",
      " [nan nan nan ... nan nan nan]\n",
      " [nan nan nan ... nan nan nan]\n",
      " [nan nan nan ... nan nan nan]]\n",
      "Total error= nan\n",
      "Predictions for test set:\n",
      " [[nan nan nan ... nan nan nan]\n",
      " [nan nan nan ... nan nan nan]\n",
      " [nan nan nan ... nan nan nan]\n",
      " ...\n",
      " [nan nan nan ... nan nan nan]\n",
      " [nan nan nan ... nan nan nan]\n",
      " [nan nan nan ... nan nan nan]]\n"
     ]
    }
   ],
   "source": [
    "#Test\n",
    "#I will now add a second layer to my initial neural network.\n",
    "#In order to do that, I will add a second array of weights\n",
    "#(Weights B obtained from my previously updated A weights).\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import random as rd\n",
    "import os\n",
    "import re\n",
    "\n",
    "cwd = os.getcwd() \n",
    "\n",
    "xy_data = pd.read_csv('./test.csv',sep=\" \") \n",
    "\n",
    "\n",
    "y_val = xy_data.iloc[:,0:1] \n",
    "x_val = xy_data.iloc[:,1:] \n",
    "\n",
    "X,Y = np.array(x_val),np.array(y_val)\n",
    "\n",
    "X = X.T \n",
    "\n",
    "\n",
    "print(X.shape, Y.shape, Y[0])\n",
    "print(xy_data.iloc[:,0].value_counts())\n",
    "\n",
    "A = np.array([[0.1], [0.2], [0.3], [0.4], [0.5], [0.1], [0.2], [0.3], [0.4], [0.5],\n",
    "              [0.1], [0.2], [0.3], [0.4], [0.5], [0.1], [0.2], [0.3], [0.4], [0.5], \n",
    "              [0.1], [0.2], [0.3], [0.4], [0.5], [0.1], [0.2], [0.3], [0.4], [0.5],\n",
    "              [0.1], [0.2], [0.3], [0.4], [0.5], [0.1], [0.2], [0.3], [0.4], [0.5],\n",
    "              [0.1], [0.2], [0.3], [0.4], [0.5], [0.1], [0.2], [0.3], [0.4], [0.5],\n",
    "              [0.1], [0.2], [0.3], [0.4], [0.5], [0.1], [0.2], [0.3], [0.4], [0.5],\n",
    "              [0.1], [0.2], [0.3], [0.4], [0.5], [0.1], [0.2], [0.3], [0.4], [0.5],\n",
    "              [0.1], [0.2], [0.3], [0.4], [0.5], [0.1], [0.2], [0.3], [0.4], [0.5],\n",
    "              [0.1], [0.2], [0.3], [0.4], [0.5], [0.1], [0.2], [0.3], [0.4], [0.5],\n",
    "              [0.1], [0.2], [0.3], [0.4], [0.5], [0.1], [0.2], [0.3], [0.4], [0.5]])\n",
    "A = A.T\n",
    "#print (A.shape) #(1, 100)\n",
    "#Shape of A = (100, 1) matrix\n",
    "#Shape of A.T = (1, 100) vector\n",
    "\n",
    "#B weights:\n",
    "B = np.array([[2.47631677],[1.36646453],[0.48526488],[-0.62961473],[0.23126217],[0.67363088],[1.52368322],[1.51473512],[0.66719809],[1.053561],[-0.23126218],[-0.62857642],[0.10910143],[1.80032682],[-0.07678844],[0.63536659],[0.00315013],[0.9597753],[-0.79438186],[1.36463341],[0.59103477],[-0.10910145],[-0.43191581],[0.97274755],[-0.52987776],[-0.00248008],[0.2],[1.62982099],[-1.82898889],[-0.70986642],[-0.00315012],[1.32004632],[-0.70986642],[0.62843979],[0.5],[0.12063146],[0.86719809],[-0.89510681],[2.04777785],[-0.10365767],[-0.15],[0.2],[0.3],[5.47565916e+142],[1.84265348],[-0.65411806],[0.79123172],[-0.36891434],[0.92366903],[-0.14994765],[-0.46675497],[0.29717401],[0.3],[0.63353545],[0.5],[0.1],[0.2],[0.3],[1.62295058],[0.72252009],[-0.98342359],[1.52443401],[-0.29717366],[0.15],[0.5],[0.1],[0.62961473],[0.3],[0.97274755],[0.5],[0.59028988],[-0.92366905],[0.94616214],[0.4],[0.5],[-0.72252009],[0.70986641],[0.37457371],[-0.28861552],[0.5],[1.78696124],[-0.85704485],[0.36126004],[-0.66719809],[0.5],[0.1],[0.2],[0.96841101],[-0.33191581],[-0.07678844],[1.32295058],[1.93023552],[0.79438185],[0.4],[0.93259262],[0.1],[0.2],[0.3],[-0.36126004],[1.15225964]])\n",
    "B = B.T\n",
    "#print(B.shape) #(1, 100) with B.T, \n",
    "#Shape of B = (1, 100) matrix\n",
    "#Shape of B.T = (100, 1) vector\n",
    "\n",
    "\n",
    "def ReLU(X):\n",
    "    return (X >= 0)*X\n",
    "def derivReLU(X):\n",
    "    return 1*(X > 0)\n",
    "\n",
    "#summary:\n",
    "#print(\"X\", X.shape) #(100,50) \n",
    "#print(\"Y\", Y.shape) #(50,1)\n",
    "#print(\"A\", A.shape) #(1,100)\n",
    "#print(\"B\", B.shape) #(1,100)\n",
    "\n",
    "alpha = 0.002 \n",
    "Ni =  30\n",
    "\n",
    "for iteration in range(Ni):\n",
    "    for ind in range(50):\n",
    "        input_data = X[:, [ind]] \n",
    "        goal = Y[ind]\n",
    "        pre_layer1 =  np.matmul(A, input_data)\n",
    "        layer1 =  ReLU(pre_layer1)   \n",
    "        pred =  np.matmul(B.T, layer1)   \n",
    "        gap = pred - goal\n",
    "        error = gap**2\n",
    "        # Updating second layer's weights\n",
    "        B = B - alpha * gap*layer1.T\n",
    "        # Updating first layer's weights in two steps\n",
    "        aux = np.multiply(B.T, derivReLU(pre_layer1)) #Multiply arguments element-wise, B and derivReLU evaluated at Ax\n",
    "        A =  A - alpha * gap* np.matmul(aux, input_data)#update A\n",
    "print('Final A weights:\\n', A,'\\n',\n",
    "      'Final B weights:\\n', B)\n",
    "\n",
    "pred = np.matmul(B, ReLU(np.matmul(A, X)))#final predictions\n",
    "gap2 = (pred- Y.T)**2\n",
    "total_error = np.sum(gap2)\n",
    "print('Total error=', total_error)\n",
    "print('Predictions for test set:\\n', pred)\n",
    "#I'm not sure why I get a RuntimeWarning: overflow encountered in (function).\n",
    "#I downloaded the latest version of the data and \n",
    "#I checked the test set, and numbers are not too big but it's still an error. \n",
    "#I tried also reducing the numbers of the B weights but it didnt work.\n",
    "#Reduced B weights:\n",
    "#B = np.array([[0.24],[0.13],[0.48],[-0.62],[0.23],[0.67],[0.15],[0.15],[0.66],[0.10],[-0.23],[-0.62],[0.10],[0.18],[-0.07],[0.63],[0.003],[0.95],[-0.79],[0.13],[0.59],[-0.10],[-0.43],[0.97],[-0.52],[-0.002],[0.2],[0.16],[-0.18],[-0.70],[-0.003],[0.13],[-0.70],[0.62],[0.5],[0.12],[0.86],[-0.89],[0.20],[-0.10],[-0.15],[0.2],[0.3],[0.54],[0.18],[-0.65],[0.79],[-0.36],[0.92],[-0.14],[-0.46],[0.29],[0.3],[0.63],[0.5],[0.1],[0.2],[0.3],[0.16],[0.72],[-0.98],[0.15],[-0.29],[0.15],[0.5],[0.1],[0.62],[0.3],[0.97],[0.5],[0.59],[-0.92],[0.94],[0.4],[0.5],[-0.72],[0.70],[0.37],[-0.28],[0.5],[0.17],[-0.85],[0.36],[-0.66],[0.5],[0.1],[0.2],[0.96],[-0.33],[-0.07],[0.13],[0.19],[0.79],[0.4],[0.93],[0.1],[0.2],[0.3],[-0.36],[0.11]])\n",
    "#Finally, I managed to make it work in the cell below by setting a very low\n",
    "#alpha value. This would not work with the B weights in this cell but it works with the second ones\n",
    "#that I have in the notation above. The results are in the following cell:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 50) (50, 1) [1]\n",
      "0    26\n",
      "1    24\n",
      "Name: true_value, dtype: int64\n",
      "Final A weights:\n",
      " [[-0.26090742 -0.16090742 -0.06090742 ... -0.06090742  0.03909258\n",
      "   0.13909258]\n",
      " [-0.20711853 -0.10711853 -0.00711853 ... -0.00711853  0.09288147\n",
      "   0.19288147]\n",
      " [-0.36700663 -0.26700663 -0.16700663 ... -0.16700663 -0.06700663\n",
      "   0.03299337]\n",
      " ...\n",
      " [-0.33405336 -0.23405336 -0.13405336 ... -0.13405336 -0.03405336\n",
      "   0.06594664]\n",
      " [-0.36685485 -0.26685485 -0.16685485 ... -0.16685485 -0.06685485\n",
      "   0.03314515]\n",
      " [-0.20799761 -0.10799761 -0.00799761 ... -0.00799761  0.09200239\n",
      "   0.19200239]] \n",
      " Final B weights:\n",
      " [[-0.01272134 -0.26780102  0.42279986 ...  0.14328221 -0.43806785\n",
      "  -0.29947727]\n",
      " [ 0.15662833  0.06093492  0.45372846 ...  0.2385259  -0.39470441\n",
      "   0.04253175]\n",
      " [-0.40361068 -1.0467181   0.3547207  ... -0.0710054  -0.53393815\n",
      "  -1.11032501]\n",
      " ...\n",
      " [-0.11104759 -0.4656654   0.40583458 ...  0.08979681 -0.46195136\n",
      "  -0.50549003]\n",
      " [ 0.86610466  1.42246278  0.5890221  ...  0.64283752 -0.20613627\n",
      "   1.45824633]\n",
      " [ 0.18710374  0.12008927  0.45932919 ...  0.25567834 -0.38686037\n",
      "   0.10407137]]\n",
      "Total error= 17509.811677565693\n",
      "Predictions for test set:\n",
      " [[-0.20541109 -1.34699736  0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 0.06125839  0.26358874  0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [-0.83533164 -5.13704731  0.         ...  0.          0.\n",
      "   0.        ]\n",
      " ...\n",
      " [-0.36526158 -2.30748902  0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 1.16640165  6.94315808  0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 0.10923548  0.55329189  0.         ...  0.          0.\n",
      "   0.        ]]\n"
     ]
    }
   ],
   "source": [
    "#Test\n",
    "#The cumulative error is rather high\n",
    "#this might be because I over trained the training set \n",
    "#so the process does not work good with new data\n",
    "#It could be because the process is done in a wrong way as well.\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import random as rd\n",
    "import os\n",
    "import re\n",
    "\n",
    "cwd = os.getcwd() \n",
    "\n",
    "xy_data = pd.read_csv('./test.csv',sep=\" \") \n",
    "\n",
    "\n",
    "y_val = xy_data.iloc[:,0:1] \n",
    "x_val = xy_data.iloc[:,1:] \n",
    "\n",
    "X,Y = np.array(x_val),np.array(y_val)\n",
    "\n",
    "X = X.T \n",
    "\n",
    "\n",
    "print(X.shape, Y.shape, Y[0])\n",
    "print(xy_data.iloc[:,0].value_counts())\n",
    "\n",
    "A = np.array([[0.1], [0.2], [0.3], [0.4], [0.5], [0.1], [0.2], [0.3], [0.4], [0.5],\n",
    "              [0.1], [0.2], [0.3], [0.4], [0.5], [0.1], [0.2], [0.3], [0.4], [0.5], \n",
    "              [0.1], [0.2], [0.3], [0.4], [0.5], [0.1], [0.2], [0.3], [0.4], [0.5],\n",
    "              [0.1], [0.2], [0.3], [0.4], [0.5], [0.1], [0.2], [0.3], [0.4], [0.5],\n",
    "              [0.1], [0.2], [0.3], [0.4], [0.5], [0.1], [0.2], [0.3], [0.4], [0.5],\n",
    "              [0.1], [0.2], [0.3], [0.4], [0.5], [0.1], [0.2], [0.3], [0.4], [0.5],\n",
    "              [0.1], [0.2], [0.3], [0.4], [0.5], [0.1], [0.2], [0.3], [0.4], [0.5],\n",
    "              [0.1], [0.2], [0.3], [0.4], [0.5], [0.1], [0.2], [0.3], [0.4], [0.5],\n",
    "              [0.1], [0.2], [0.3], [0.4], [0.5], [0.1], [0.2], [0.3], [0.4], [0.5],\n",
    "              [0.1], [0.2], [0.3], [0.4], [0.5], [0.1], [0.2], [0.3], [0.4], [0.5]])\n",
    "A = A.T\n",
    "#print (A.shape) #(1, 100)\n",
    "#Shape of A = (100, 1) matrix\n",
    "#Shape of A.T = (1, 100) vector\n",
    "\n",
    "#B weights:\n",
    "B = np.array([[0.24],[0.13],[0.48],[-0.62],[0.23],[0.67],[0.15],[0.15],[0.66],[0.10],[-0.23],[-0.62],[0.10],[0.18],[-0.07],[0.63],[0.003],[0.95],[-0.79],[0.13],[0.59],[-0.10],[-0.43],[0.97],[-0.52],[-0.002],[0.2],[0.16],[-0.18],[-0.70],[-0.003],[0.13],[-0.70],[0.62],[0.5],[0.12],[0.86],[-0.89],[0.20],[-0.10],[-0.15],[0.2],[0.3],[0.54],[0.18],[-0.65],[0.79],[-0.36],[0.92],[-0.14],[-0.46],[0.29],[0.3],[0.63],[0.5],[0.1],[0.2],[0.3],[0.16],[0.72],[-0.98],[0.15],[-0.29],[0.15],[0.5],[0.1],[0.62],[0.3],[0.97],[0.5],[0.59],[-0.92],[0.94],[0.4],[0.5],[-0.72],[0.70],[0.37],[-0.28],[0.5],[0.17],[-0.85],[0.36],[-0.66],[0.5],[0.1],[0.2],[0.96],[-0.33],[-0.07],[0.13],[0.19],[0.79],[0.4],[0.93],[0.1],[0.2],[0.3],[-0.36],[0.11]])\n",
    "B = B.T\n",
    "#print(B.shape) #(1, 100) with B.T, \n",
    "#Shape of B = (1, 100) matrix\n",
    "#Shape of B.T = (100, 1) vector\n",
    "\n",
    "\n",
    "def ReLU(X):\n",
    "    return (X >= 0)*X\n",
    "def derivReLU(X):\n",
    "    return 1*(X > 0)\n",
    "\n",
    "#summary:\n",
    "#print(\"X\", X.shape) #(100,50) \n",
    "#print(\"Y\", Y.shape) #(50,1)\n",
    "#print(\"A\", A.shape) #(1,100)\n",
    "#print(\"B\", B.shape) #(1,100)\n",
    "\n",
    "alpha = 0.0002 \n",
    "Ni =  300\n",
    "\n",
    "for iteration in range(Ni):\n",
    "    for ind in range(50):\n",
    "        input_data = X[:, [ind]] \n",
    "        goal = Y[ind]\n",
    "        pre_layer1 =  np.matmul(A, input_data)\n",
    "        layer1 =  ReLU(pre_layer1)   \n",
    "        pred =  np.matmul(B.T, layer1)   \n",
    "        gap = pred - goal\n",
    "        error = gap**2\n",
    "        # Updating second layer's weights\n",
    "        B = B - alpha * gap*layer1.T\n",
    "        # Updating first layer's weights in two steps\n",
    "        aux = np.multiply(B.T, derivReLU(pre_layer1)) #Multiply arguments element-wise, B and derivReLU evaluated at Ax\n",
    "        A =  A - alpha * gap* np.matmul(aux, input_data)#update A\n",
    "print('Final A weights:\\n', A,'\\n',\n",
    "      'Final B weights:\\n', B)\n",
    "\n",
    "pred = np.matmul(B, ReLU(np.matmul(A, X)))#final predictions\n",
    "gap2 = (pred- Y.T)**2\n",
    "total_error = np.sum(gap2)\n",
    "print('Total error=', total_error)\n",
    "print('Predictions for test set:\\n', pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "96a55daf58faaef8a1b96e459858a5fcbfc9b90e0725bdbc6027ea64829581b1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
